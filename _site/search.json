[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "webR via Quarto Extensions\n\n\n\n\n\n\n\ntidyverse\n\n\nwebR\n\n\n\n\nwebR has been on my mind recently… With the recent release of webR 0.2.0, it’s time to dive in!\n\n\n\n\n\n\nAug 21, 2023\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nImpressing Hiring Teams with a Shiny App Demo\n\n\n\n\n\n\n\ntidyverse\n\n\nshiny\n\n\nrenv\n\n\nbslib\n\n\n\n\nLast week, I presented to SoCal RUG on how to build a Shiny app demo as a resume or cover letter accessory. This ‘accessory’ will help impress hiring managers that you’re interviewing with and also differentiate you from other data science applicants.\n\n\n\n\n\n\nMar 28, 2023\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nR Goodies for 2023\n\n\n\n\n\n\n\ntidymodels\n\n\nprobably\n\n\narrow\n\n\nduckdb\n\n\nsodium\n\n\ncyphr\n\n\n\n\nDespite all of the noise of AI-generated art and ChatGPT, 2022 was a great year for data science. These are some of my favorite tricks that I will continue to explore in 2023.\n\n\n\n\n\n\nJan 15, 2023\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nData Science Hangout\n\n\n\n\n\n\n\ntidyverse\n\n\nshiny\n\n\nmachine learning\n\n\ntidymodels\n\n\n\n\nI was recently a guest on Posit’s Data Science Hangout, hosted by Rachael Dempsey, and I had a blast. This is the recording where I talked about my transition from Excel-based financial modeling to machine learning at scale with R.\n\n\n\n\n\n\nJan 1, 2023\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nBuild a Shiny App Demo\n\n\n\n\n\n\n\nshiny\n\n\ngithub\n\n\nbslib\n\n\ndocker\n\n\n\n\nDuring my interview process with Bloomreach, I developed an interactive Shiny app styled with the Bloomreach branding and color scheme. This tutorial will help you do the same to demo the power of Shiny to prospective employers.\n\n\n\n\n\n\nNov 15, 2022\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nTorch for R + luz\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\nThe ‘torch for R’ ecosystem is a collection of extensions for torch, an R framework for machine learning and artificial intelligence based on PyTorch.\n\n\n\n\n\n\nOct 22, 2021\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nBeautiful Maps with Rayshader\n\n\n\n\n\n\n\nrayshader\n\n\nggplot2\n\n\nvisualizations\n\n\n\n\nIf you love beautiful 2D and 3D maps, now you can create your own with elevation data, the rayshader package, OpenStreetMap, and ggplot2.\n\n\n\n\n\n\nSep 7, 2021\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nA Brief History of the Dataframe\n\n\n\n\n\n\n\ntibble\n\n\n\n\nInsightful read sourced from Towards Data Science on the history of the dataframe - From its origins in the S programming language, to R, to pandas for Python (to the tibble for R).\n\n\n\n\n\n\nMay 24, 2021\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nDT: An R Interface to the JavaScript library DataTables\n\n\n\n\n\n\n\ndt\n\n\n\n\nThe R package DT provides an R interface to the JavaScript library DataTables. R data objects (matrices or data frames) can be displayed as tables on HTML pages, and DataTables provides filtering, pagination, sorting, interactivity with Shiny, and many other features.\n\n\n\n\n\n\nMay 3, 2021\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nrstudio::global tips, tricks, and more\n\n\n\n\n\n\n\ntidymodels\n\n\n\n\nRStudio’s annual conference saw roughly 17,000 attendees for their first global, all-virtual, 24-hour event. I attended several sessions throughout the day and I’ll highlight my favorite data bytes learned that day. I’ll also share relatable content for better modeling with R.\n\n\n\n\n\n\nJan 24, 2021\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nrsthemes: Customizing your RStudio IDE\n\n\n\n\n\n\n\nquick tips\n\n\n\n\nChange up your editor theme with {rsthemes}, a collection of themes to freshen up the RStudio IDE aesthetics.\n\n\n\n\n\n\nJan 19, 2021\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nPreparing for 2021 Goals with a Raspberry Pi\n\n\n\n\n\n\n\nlinux\n\n\n\n\nI have been planning my 2021 goals and the Raspberry Pi 4 will help me kill a few birds with one stone.\n\n\n\n\n\n\nDec 31, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nOne-Hour R Package Development Tutorial by Shannon Pileggi, PhD\n\n\n\n\n\n\n\npackages\n\n\n\n\nI followed this tutorial and created a package successfully. It took 45 minutes. Follow, this, guide, my fellow R friends!\n\n\n\n\n\n\nDec 19, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nWhat’s New in dbplyr 2.0.0\n\n\n\n\n\n\n\ntidyverse\n\n\ndplyr\n\n\n\n\ndbplyr, a database backend for dplyr, just released v2.0.0 today… Awesome stuff here!\n\n\n\n\n\n\nNov 4, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nDetailed R Package Development Tutorial from Method Bites\n\n\n\n\n\n\n\npackages\n\n\n\n\nThis is one of the best, most detailed, how-to guides for developing your own R package from A-to-Z\n\n\n\n\n\n\nSep 13, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nShiny Voice-Activated input\n\n\n\n\n\n\n\nshiny\n\n\n\n\nWant to make your Shiny apps voice-interactive? Now it’s possible.\n\n\n\n\n\n\nJul 3, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nTesting dplyr 1.0.0\n\n\n\n\n\n\n\ntidyverse\n\n\ndplyr\n\n\n\n\nFinally getting around to trying out dplyr 1.0.0… Love it!\n\n\n\n\n\n\nJun 20, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nLying with Statistics\n\n\n\n\n\n\n\nstatistics\n\n\nggplot2\n\n\nvisualizations\n\n\n\n\nIt’s quite easy to manipulate raw data in a manner that ‘proves’ your point. For the sake of exploring this topic further, I’ll analyze police killing data and present it in three different ways.\n\n\n\n\n\n\nApr 19, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nD&D Adventures + blogdown\n\n\n\n\n\n\n\nrmarkdown\n\n\nblogdown\n\n\n\n\nIt’s an incredible time to be creative! I’ve recently been re-learning Dungeons & Dragons with a group of friends and having so much fun in the process.\n\n\n\n\n\n\nApr 5, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nthemis: Extra Steps for tidymodels + recipes\n\n\n\n\n\n\n\ntidymodels\n\n\n\n\nthemis contain extra steps for the recipes package for dealing with unbalanced data. The name themis is that of the ancient Greek goddess who is typically depicted with a balance.\n\n\n\n\n\n\nFeb 20, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nCopy R Objects to Clipboard\n\n\n\n\n\n\n\nquick tips\n\n\n\n\nI was today years old when I learned that you could easily copy/paste R objects to your clipboard.\n\n\n\n\n\n\nFeb 8, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nHadley Wickham on SuperDataScience Podcast\n\n\n\n\n\n\n\npodcast\n\n\n\n\nMy eyes were opened to the world of analytics and data science in part through Kirill Eremenko and his amazing SuperDataScience podcast.\n\n\n\n\n\n\nFeb 4, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nResampling with k-fold Cross Validation\n\n\n\n\n\n\n\nstatistics\n\n\nmachine learning\n\n\n\n\nClick for a high-level recap of k-fold cross validation as a resampling method.\n\n\n\n\n\n\nFeb 1, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nAn Algorithm for Better Espresso\n\n\n\n\n\n\n\nstatistics\n\n\n\n\nCoffee lovers… You might find this study fascinating. Finally, a model for the perfect cup of espresso!\n\n\n\n\n\n\nJan 22, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing Earthquakes in Puerto Rico\n\n\n\n\n\n\n\nstatistics\n\n\nggplot2\n\n\nvisualizations\n\n\n\n\nPuerto Rico has been experiencing atypical seismic activity since November 2019.\n\n\n\n\n\n\nJan 20, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nCreate Your Own R Package\n\n\n\n\n\n\n\npackages\n\n\n\n\nIf you are interested in developing your own R packages, this thorough A-to-Z tutorial resource is one to bookmark.\n\n\n\n\n\n\nJan 18, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nWriting to Excel from R\n\n\n\n\n\n\n\nquick tips\n\n\n\n\nSpending your days R but working closely with analysts and leadership that live in Excel? Get familiar with openxlsx.\n\n\n\n\n\n\nJan 9, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nCalifornia Consumer Privacy Act\n\n\n\n\n\n\n\nprivacy\n\n\n\n\nThe California Consumer Privacy Act is set to go into effect on January 1, 2020. The CCPA, similar in nature to GDPR, will provide California residents with new consumer data rights.\n\n\n\n\n\n\nJan 4, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nHoliday Expectations vs Reality\n\n\n\n\n\nHoliday expectations…\n\n\n\n\n\n\nJan 1, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nUbuntu Update to 19.10\n\n\n\n\n\n\n\nlinux\n\n\n\n\nThings not to do with Linux, learned the hard way, as we approach the end of 2019…\n\n\n\n\n\n\nDec 31, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nBook Rec: Hands-On Machine Learning with R\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\nDigging into Hands-On Machine Learning with R by Brad Boehmke, PhD, and Brandon Greenwell.\n\n\n\n\n\n\nDec 17, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nSpreading Joy with Java Joy\n\n\n\n\n\n\n\nsocial good\n\n\n\n\nBay Area friends… Next time your organization needs coffee for an event, quarterly meeting, all-hands, or lunch and learn, reach out to Java Joy!\n\n\n\n\n\n\nDec 15, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nEducation Reform and Lessons from Finland\n\n\n\n\n\n\n\nstatistics\n\n\n\n\nQuartz and Google’s Avinash Kaushik share insights comparing national education systems, test performance, and time spent on learning.\n\n\n\n\n\n\nDec 4, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a Website with Blogdown\n\n\n\n\n\n\n\nrmarkdown\n\n\nblogdown\n\n\n\n\nLearn how to build and deploy a website with R, blogdown, GitHub, Hugo, and Netlify.\n\n\n\n\n\n\nNov 18, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n15 Tips for Making Better Use of R\n\n\n\n\n\n\n\nrmarkdown\n\n\n\n\nIf you are an R user not using R Markdown, these tricks could be helpful for you.\n\n\n\n\n\n\nNov 11, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nShiny Developer Expert Course\n\n\n\n\n\n\n\nshiny\n\n\n\n\nMatt Dancho and Business Science introduce a new R Shiny Expert Developer course.\n\n\n\n\n\n\nNov 5, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nVisual & Data Journalism at BBC\n\n\n\n\n\n\n\nggplot2\n\n\nvisualizations\n\n\n\n\nThe BBC News data science and visualization team published this great overview on their analytics and visualizations lessons learned over the course of 1.5 years.\n\n\n\n\n\n\nOct 22, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\nInformation is Beautiful 2019 Awards\n\n\n\n\n\n\n\nvisualizations\n\n\n\n\nVisualization professionals present their best at the annual Information is Beautiful Awards show.\n\n\n\n\n\n\nOct 19, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Tulum, Quintana Roo, Mexico - The second happiest place on Earth, after Disneyland"
  },
  {
    "objectID": "about.html#professional",
    "href": "about.html#professional",
    "title": "About Me",
    "section": "Professional",
    "text": "Professional\nMy career includes over 15 years of data science, financial strategy, and business analytics for EY, PG&E, KPMG, Centene, and Bloomreach. I use the R and Python programming languages on a daily basis but am much more advanced with my R work. My teams apply data science techniques to empower decision makers with descriptive, predictive, and prescriptive analytics. I help bridge the gap between business needs and IT to develop and deploy automation solutions, predictive algorithms, and interactive web apps."
  },
  {
    "objectID": "about.html#scatter-podcast",
    "href": "about.html#scatter-podcast",
    "title": "About Me",
    "section": "Scatter Podcast",
    "text": "Scatter Podcast\nIn 2019, I launched Scatter Podcast to share career tips and insights from data science leaders for students, business managers, and professionals looking to pivot into data science. It was a fun and incredibly rewarding side project but after 30 episodes, I started to feel fatigued with the amount of time it took to plan, record, edit, market, etc. The podcast is on hold but not dead!"
  },
  {
    "objectID": "about.html#programming-analytics-skills",
    "href": "about.html#programming-analytics-skills",
    "title": "About Me",
    "section": "Programming & Analytics Skills",
    "text": "Programming & Analytics Skills\n\nR: tidyverse, tidymodels, Quarto, Shiny, Plotly, Leaflet, Prophet, xaringan, XGBoost, Ranger, and many more\nOther Analytics & Programming Platforms: Python, Jupyter Notebooks, h2o, GitLab / GitHub, RStudio Connect, AWS S3, AWS Redshift, Google Cloud Platform, BigQuery, SQL, Netlify\nMicrosoft: Expert-level Excel financial modeler with experiences including nested formulas, XLOOKUPs, Solver for optimization and simulations (yes… in Excel!), macros for psuedo-automated data manipulation, add-in integrations, and more"
  },
  {
    "objectID": "about.html#media-appearances",
    "href": "about.html#media-appearances",
    "title": "About Me",
    "section": "Media Appearances",
    "text": "Media Appearances\n\nUC Irvine’s inaugural Latinx Initiative Conference (2021-10-15)\nData Points: Healthcare and Finance virtual conference hosted by Grid Dynamics (2021-06-09)\n\nWatch my presentation Healthcare Finance Beyond Excel: R for Automation, Time Series Forecasting, and Interactive Reporting\n\nScatter Podcast on UC Irvine News (2019-06-13)\nWinner of the Orange County Predictive Modeling Hackathon (2019-05-20)\nScatter Podcast mention on Forbes (2019-04-14)"
  },
  {
    "objectID": "posts/2020-12-31-Raspberry-Pi/2020-12-31-Raspberry-Pi.html",
    "href": "posts/2020-12-31-Raspberry-Pi/2020-12-31-Raspberry-Pi.html",
    "title": "Preparing for 2021 Goals with a Raspberry Pi",
    "section": "",
    "text": "Killing a few birds with one stone and getting ready for 2021… 1) Getting better at Linux / shell commands,  2) Learning Julia (see my Resources tab for some great Julia starter links!),  3) Doing it all on a Raspberry Pi 😜 Happy holidays and Happy New Years to all!\nBelow are some pics of my current progress.\n\n\n\nRaspberry Pi 4, Model B, 8GB RAM, & Super Cute Tiny Heat Sinks\n\n\n\n\n\nEnclosing the Pi 4 in a CanaKit case + fan\n\n\n\n\n\nRaspberry Pi OS, a Debian distro for the Pi\n\n\n\n\n\nJupyter Lab server on the Pi set up w/ Python, R, and Julia\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2021-05-24-dataframe/2021-05-24-dataframe.html",
    "href": "posts/2021-05-24-dataframe/2021-05-24-dataframe.html",
    "title": "A Brief History of the Dataframe",
    "section": "",
    "text": "This is quite the insightful read on the history of the dataframe. From its origins in the S programming language, to R, to the pandas library for Python (to the tibble for R!)… I couldn’t imagine doing my work without this fundamental data structure, but will it really be defined out of existence? See below link for an excellent overview by Devin Petersohn:\n\nSource:\n\nTowards Data Science: Preventing the Death of the Dataframe\npandas for Python\ntibble for R\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-01-09-Writing-to-Excel-from-R/2020-01-09-Writing-to-Excel-from-R.html",
    "href": "posts/2020-01-09-Writing-to-Excel-from-R/2020-01-09-Writing-to-Excel-from-R.html",
    "title": "Writing to Excel from R",
    "section": "",
    "text": "openxlsx is an awesome package for writing multiple datasets to multiple sheets of an Excel workbook, and allows for robust styling and cell formatting. I found a tutorial on RPubs written by Ezekiel Adebayo Ogundepo covering this package that I often use at work. Definitely bookmark worthy.\nHere is a high-level code example:\nlibrary(openxlsx)\n\n# Name the Excel worksheets where you want to export data to\nlist_of_datasets &lt;- list(\"Overall Summary\" = df_1, \"LOB Summary\" = df_2, \"Details\" = df_3)\n\n# Save the Excel workbook to your wd with your worksheet list and name the Excel file\nwrite.xlsx(list_of_datasets, \"Excel_Workbook_2017.01.20.xlsx\")\nLinks:\n\nEzekiel’s openxlsx tutorial on RPubs\nopenxlsx repository on GitHub\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-02-04-Hadley-Wickham-on-SDS-Podcast/2020-02-04-Hadley-Wickham-on-SDS-Podcast.html",
    "href": "posts/2020-02-04-Hadley-Wickham-on-SDS-Podcast/2020-02-04-Hadley-Wickham-on-SDS-Podcast.html",
    "title": "Hadley Wickham on SuperDataScience Podcast",
    "section": "",
    "text": "Hands down, he was the inspiration for me starting my own podcast and I still listen to SDS weekly. Being a massive fan of RStudio, I’m super excited about this most recent SDS episode featuring Hadley Wickham, RStudio’s Chief Data Scientist and creator of ggplot2, dplyr, author, professor… he’s a research and data science genius!.\nThis episode is a great listen as he explores RStudio Python and R integration and the future of data science.\n\nSource:\n\nSuperDataScience ep.337 with Hadley Wickham\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-01-18-Create-R-Packages/2020-01-18-Create-R-Packages.html",
    "href": "posts/2020-01-18-Create-R-Packages/2020-01-18-Create-R-Packages.html",
    "title": "Create Your Own R Package",
    "section": "",
    "text": "“This book espouses our philosophy of package development: anything that can be automated, should be automated. Do as little as possible by hand. Do as much as possible with functions.” Great quote by Hadley Wickham and Jenny Bryan in their book “R Packages: Organize, Test, Document and Share Your Code,” also available online (link below).\nA coworker sent me this link and I’m super excited to start developing some packages. Great timing as one of my 2020 resolutions is to write my own package(s)!\n\nLinks:\n\nR Packages by Hadley Wickham and Jenny Bryan\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-11-18-Blogdown/2019-11-18-Blogdown.html",
    "href": "posts/2019-11-18-Blogdown/2019-11-18-Blogdown.html",
    "title": "Building a Website with Blogdown",
    "section": "",
    "text": "Pablo Barajas, a recent graduate at UC Irvine and friend of Scatter Podcast, led a presentation on how to build a website using R, RStudio, and Blogdown.\nFor anyone wanting to fairly easily build an online portfolio or blog that looks professional, I highly recommend Blogdown. I used this process to build this website, as did Pablo for his personal website, and I can help if you have any questions. Here’s the process in short: RStudio site build with Blogdown -&gt; Commit to GitHub -&gt; Apply Hugo theme -&gt; Netlify for CI/CD\nThis event was hosted by the Orange County R Users Group (“OCRUG”) and UCI’s Merage Analytics Club (“MAC”). If you’re in Orange County and looking to network and learn with local data science practitioners and students, I highly recommend attending OCRUG events, R-Ladies Irvine events, or public MAC events. Great learning opportunities here!\nLinks:\n\nPablo’s presentation on his Blogdown site\nBlogdown overview from RStudio\nOCRUG’s homepage\nR-Ladies Irvine chapter\nMerage Analytics Club website\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2021-05-03-DT/2021-05-03-DT.html",
    "href": "posts/2021-05-03-DT/2021-05-03-DT.html",
    "title": "DT: An R Interface to the JavaScript library DataTables",
    "section": "",
    "text": "The DT package for R is a powerful integration of the JavaScript library DataTables. One of the things that I enjoy with DT is enhancing my Shiny apps by rendering interactive tables with DT. The DT documentation, written by RStudio PBC and the DT devs, is incredibly well-written and you’ll learn a lot about how you can use these tables in the development of robust R web apps. And the available Options…. Endless in a good way!\nIf you’re a Shiny dev, check out the live demos sourced below.\n\nSource:\n\nDT on GitHub\nDT site\nLive Shiny demo: DT Selections Ex. 1\nLive Shiny demo: DT Selection Ex. 2\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2023-08-21-webr-via-quarto-extensions/index.html",
    "href": "posts/2023-08-21-webr-via-quarto-extensions/index.html",
    "title": "webR via Quarto Extensions",
    "section": "",
    "text": "WebAssembly\nImagine you’ve written a program in R. It’s simple and does exactly what you want but now, you wish to make it available for many people on the web, allowing them to run it within their web browsers. The challenge is that web browsers don’t natively understand R code.\nTraditionally, web browsers understand JavaScript as the primary language to run any dynamic operations. If you wanted to run code in a browser, you’d need to rewrite it in JavaScript. Re-writing code for every language to JavaScript is tedious and not efficient. This is where WebAssembly (or “wasm”) comes into play.\nWebAssembly introduces a new type of code that can be run in modern web browsers. It is designed as a low-level virtual machine that runs code at near-native speed. What’s fascinating about WebAssembly is that it isn’t written by hand; instead, other languages (like C, C++, Rust, Python, R, and more) can be compiled to WebAssembly.\nIn essence, WebAssembly allows you to take code from languages other than JavaScript, compile it into a format that the browser can understand, and then run it efficiently.\n\n\nwebR\nCompiling R to wasm was hypothetical even just a few years ago but is now possible via webR. Being able to run R functions within a web browser without the need for an R interpreter on the user’s end feels game changing.\n\nBeing able to run R functions within a web browser without the need for an R interpreter on the user’s end feels game changing.\n\nwebR is enabling R developers to bring their applications, algorithms, and visualizations to broader audiences via the web. R + wasm is definitely in the early stages of development and new use cases are being released daily by the data science and web development communities. For now, I’m just enjoying learning the basics and thinking about potential use cases.\n\n\nDemo\nFor the examples below, I’m running webR via a Quarto extension. I’m following James Balamuta’s documentation on GitHub to learn how to use his webR Code Extension for Quarto before I dive deeper into the internals of webR. I’m basically using a plug-and-play implementation of webR for Quarto 🚀. My goal is to start here but eventually dive deeper into the core webR package and framework.\nThis is a webr-enabled code cell in a Quarto HTML document.\nfit = lm(mpg ~ am, data = mtcars)\nsummary(fit)\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-01-01-Holidays/2020-01-01-Holidays.html",
    "href": "posts/2020-01-01-Holidays/2020-01-01-Holidays.html",
    "title": "Holiday Expectations vs Reality",
    "section": "",
    "text": "Holiday expectation: Going to finish a few books, an online class that’s been lingering, research…\nHoliday reality: Make a million espressos, eat loads of chocolate and cake, play more ‘The Legend of Zelda: Link’s Awakening’ than I thought was humanly possible…\nLife is good folks, Merry Christmas and I hope that you got to enjoy some time with family and friends this holiday season!\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html",
    "title": "Beautiful Maps with Rayshader",
    "section": "",
    "text": "R, rayshader, OpenStreetMap… Lots of amazing learning to be had with these mapping tools. I’m following Tyler Morgan-Wall PhD’s Adding Open Street Map Data to Rayshader Maps in R tutorial and learning a lot about the development of beautiful maps with R. Laguna Beach is one of my favorite cities in the USA (and the world!) so I applied the concepts from this tutorial to building 2D and 3D maps of Laguna Beach. My work-in-progress and screen captures are below.\nTyler Morgan-Wall PhD (TMW) authored the rayshader package and his step-by-step tutorial is both easy to follow and incredibly informative. If you get stuck (more like, “when you get stuck”), you can almost always find an answer on the rayshader website or GitHub repo (excellent technical documentation and code examples).\nOne last caveat before I dive into my code - Finding high-quality LIDAR or Digital Elevation Models (DEMs) for United States cities is not easy. To apply the steps below to your location of choice, you’ll have to get creative about cropping LIDAR/DEMs to zoom into the geography that you want to model (more info on cropping in the sections below)."
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#libraries",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#libraries",
    "title": "Beautiful Maps with Rayshader",
    "section": "Libraries",
    "text": "Libraries\n\n# Install dev packages\ndevtools::install_github(\"tylermorganwall/rayshader\")\ndevtools::install_github(\"tylermorganwall/rayimage\")\n\n# Load libraries\nlibrary(rayshader)\nlibrary(raster)\nlibrary(osmdata)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Source elevation data\nlaguna &lt;- raster::raster(\"./USGS_one_meter_x42y372_CA_SoCal_Wildfires_B1_2018.tif\")"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#eda",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#eda",
    "title": "Beautiful Maps with Rayshader",
    "section": "EDA",
    "text": "EDA\nTo better explore the data, lets begin by visualizing the elevation data with rayshader defaults to understand the topography that we are working with. Before we can render this visual, we need to transform the laguna data object (assigned above) to a matrix. For faster rendering and prototyping, you can also resize the matrix using rayshader::resize_matrix(). This resizing will downscale your full dataset using bilinear interpolation.\n\n# Transform the spatial data structure into a regular R matrix\nlaguna_mat &lt;- rayshader::raster_to_matrix(laguna)\n\n# Create small version of matrix for quick visual prototyping\nlaguna_small &lt;- rayshader::resize_matrix(laguna_mat, 0.1)\n\n# Plot base map using basic color defaults\nlaguna_small %&gt;% \n  rayshader::height_shade() %&gt;% \n  rayshader::plot_map()\n\n\n\n\nUSGS Laguna Canyon LIDAR"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#cropping",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#cropping",
    "title": "Beautiful Maps with Rayshader",
    "section": "Cropping",
    "text": "Cropping\nI wanted to focus on downtown Laguna Beach but this only represents a small fraction of the raw LIDAR. To get the desired lat-long range of downtown Laguna Beach, I used Google Maps to get the endpoints of my desired box. To accomplish this cropping, below, we relied on a custom cropping function that TMW shared on the rayshader site.\n\n# Crop to fit desired box around downtown Laguna Beach\nlat_range &lt;- c(33.541370925562376, 33.55945552155357)\nlong_range &lt;- c(-117.79622401045448, -117.77493476466955)\n\nconvert_coords &lt;- function(lat, long, from = CRS(\"+init=epsg:4326\"), to) {\n  data = data.frame(long = long, lat = lat)\n  sp::coordinates(data) &lt;- ~ long + lat\n  sp::proj4string(data) = from\n  # Convert to coordinate system specified by EPSG code\n  xy = data.frame(sp::spTransform(data, to))\n  colnames(xy) = c(\"x\",\"y\")\n  return(unlist(xy))\n}\n\nraster::crs(laguna)\n\nutm_bbox &lt;- convert_coords(lat = lat_range,\n                           long = long_range,\n                           to = crs(laguna))\n\nraster::extent(laguna)\n\nextent_zoomed &lt;- raster::extent(utm_bbox[1], utm_bbox[2], \n                                utm_bbox[3], utm_bbox[4])h\n\nlaguna_zoom &lt;- raster::crop(laguna, extent_zoomed)\n\nlaguna_zoom_mat &lt;- rayshader::raster_to_matrix(laguna_zoom)"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#base-map",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#base-map",
    "title": "Beautiful Maps with Rayshader",
    "section": "Base Map",
    "text": "Base Map\nI was inspired by the relief shading work of Eduard Imhof, Swiss cartographer and mapping visionary. In particular, I loved the blueish-pink palette that Imhof popularized. When you get to this step, try adjusting the settings below to your liking. We will continue to build on top of this initial base map layer.\n\nbase_map &lt;- laguna_zoom_mat %&gt;% \n  rayshader::height_shade() %&gt;% \n  rayshader::add_overlay(\n    rayshader::sphere_shade(\n      laguna_zoom_mat, \n      texture = rayshader::create_texture(\"#f5dfca\",\"#63372c\",\"#dfa283\",\"#195f67\",\"#c2d1cf\",\n                                          cornercolors = c(\"#ffc500\", \"#387642\", \"#d27441\",\"#296176\")),\n      sunangle = 0, \n      colorintensity = 5)) %&gt;%\n  rayshader::add_shadow(rayshader::lamb_shade(laguna_zoom_mat), 0.2) %&gt;%\n  rayshader::add_overlay(\n    rayshader::generate_altitude_overlay(\n      rayshader::height_shade(laguna_zoom_mat, texture = \"#91aaba\"),\n      laguna_zoom_mat,\n      start_transition = min(laguna_zoom_mat)-200,\n      end_transition = max(laguna_zoom_mat)))\n\nrayshader::plot_map(base_map)\n\n\n\n\nLaguna Beach base map a la Eduard Imhof"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#openstreetmap",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#openstreetmap",
    "title": "Beautiful Maps with Rayshader",
    "section": "OpenStreetMap",
    "text": "OpenStreetMap\nOpenStreetMap (OSM) offers an open-source, editable geographic database of the world and the underlying data includes details for roads, buildings, hiking paths, rivers, etc. We’ll leverage the osmdata package for R to import OSM data (and utilizing the overpass API). The osmdata package processes routines in C++ for fast construction and loading into R. We’ll first load several layers of OSM details. Subsequently, we’ll convert those sf objects to the coordinate system we’re working with for this rayshader project.\n\nosm_bbox = c(long_range[1],lat_range[1], long_range[2],lat_range[2])\n\nlaguna_highway &lt;- osmdata::opq(osm_bbox) %&gt;% \n  osmdata::add_osm_feature(\"highway\") %&gt;% \n  osmdata::osmdata_sf() \n\n# Transform coordinates to new projection\nlaguna_lines &lt;- sf::st_transform(laguna_highway$osm_lines,\n                                 crs = raster::crs(laguna))\n\n# View streets as a ggplot2 render \nggplot(laguna_lines, aes(color = osm_id)) + \n  geom_sf() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Open Street Map `highway` attribute in Laguna Beach\")\n\n\n\n\nOSM render of Laguna Beach roads and highways using ggplot2"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#layering",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#layering",
    "title": "Beautiful Maps with Rayshader",
    "section": "Layering",
    "text": "Layering\nTo begin, lets simply layer all the “highway” OSM attributes on top of our base map using white lines to represent the lines.\n\n# Transform sf LINESTRING geometry and create semi-transparent overlay\nbase_map %&gt;% \n  rayshader::add_overlay(\n    rayshader::generate_line_overlay(\n      laguna_lines, extent = extent_zoomed,\n      linewidth = 3, color = \"white\",\n      heightmap = laguna_zoom_mat)) %&gt;% \n  rayshader::plot_map()\n\n\n\n\nBase Map Layer 1\n\n\nI want different styles for different OSM lines so let’s continue by creating several groups of OSM lines.\n\n# Subset layers of laguna_lines\nlaguna_trails &lt;- laguna_lines %&gt;% \n  dplyr::filter(highway %in% c(\"path\", \"bridleway\", \"steps\", \"track\"))\n\nlaguna_footpaths &lt;- laguna_lines %&gt;% \n  dplyr::filter(highway %in% c(\"footway\"))\n\nlaguna_roads &lt;- laguna_lines %&gt;% \n  dplyr::filter(highway %in% c(\"unclassified\", \"primary\", \n                               \"primary_link\", \"secondary\", \n                               \"tertiary\", \"residential\", \"service\"))\n\n# Create one encompassing layer with independent element styling\ntrails_layer &lt;- \n  rayshader::generate_line_overlay(\n    \n    # Pink footpaths\n    laguna_footpaths, extent = extent_zoomed,\n    linewidth = 4, color = \"pink\",\n    heightmap = laguna_zoom_mat) %&gt;%\n  \n  # Note: While barely visible in the final outputs, the following\n  # code adds an offset black shadow to the white dashed lines to\n  # represent the hiking trails around Laguna Beach \n  rayshader::add_overlay(\n    rayshader::generate_line_overlay(\n      laguna_trails, extent = extent_zoomed,\n      linewidth = 4, color = \"black\", lty = 3, offset = c(2,-2),\n      heightmap = laguna_zoom_mat)) %&gt;%\n  rayshader::add_overlay(\n    rayshader::generate_line_overlay(\n      laguna_trails, extent = extent_zoomed,\n      linewidth = 4, color = \"white\", lty = 3,\n      heightmap = laguna_zoom_mat)) %&gt;%\n  \n  # White roads\n  rayshader::add_overlay(\n    rayshader::generate_line_overlay(\n      laguna_roads, extent = extent_zoomed,\n      linewidth = 3, color = \"white\",\n      heightmap = laguna_zoom_mat))\n\nWe’ll create some additional projections of OSM layers.\n\nlaguna_parking &lt;- osmdata::opq(osm_bbox) %&gt;% \n  osmdata::add_osm_feature(\"parking\") %&gt;% \n  osmdata::osmdata_sf() \n\nlaguna_building &lt;- osmdata::opq(osm_bbox) %&gt;% \n  osmdata::add_osm_feature(\"building\") %&gt;% \n  osmdata::osmdata_sf()\n\nlaguna_parking_poly &lt;- st_transform(laguna_parking$osm_polygons, crs = crs(laguna))\nlaguna_building_poly &lt;- st_transform(laguna_building$osm_polygons, crs = crs(laguna))"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#final-touches",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#final-touches",
    "title": "Beautiful Maps with Rayshader",
    "section": "Final Touches",
    "text": "Final Touches\nNow we’re ready to build upon our base map and see the 2D visual in action. Let’s create a polygon layer of the parking and building features, and adding the styled trails_layer on top of that.\n\npolygon_layer &lt;- \n  rayshader::generate_polygon_overlay(laguna_parking_poly, \n                                      extent = extent_zoomed,\n                                      heightmap = laguna_zoom_mat, \n                                      palette = \"grey30\") %&gt;%\n  rayshader::add_overlay(\n    rayshader::generate_polygon_overlay(laguna_building_poly, \n                                        extent = extent_zoomed,\n                                        heightmap = laguna_zoom_mat, \n                                        palette = \"lightgrey\"))\n\nbase_map %&gt;% \n  rayshader::add_overlay(polygon_layer) %&gt;%\n  rayshader::add_overlay(trails_layer) %&gt;%\n  rayshader::plot_map()\n\n\n\n\n2D render of Laguna Beach… Success!"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#features",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#features",
    "title": "Beautiful Maps with Rayshader",
    "section": "Features",
    "text": "Features\nTo render a 3D map, we follow a very similar pipeline to the 2D map but utilizing rayshader::plot_3d(). This function introduces new arguments and I spent much more time than I had anticipated in adjusting the zscale, zoom, rotation, pitch, and water specifications.\nTo exaggerate the elevation details in this area, I decided on a zscale &lt; 1 that would still resemble the reality and beauty of the Laguna Beach hills. Too low of a zscale, e.g., zscale = 0.25, was too exaggerated for my liking. Once the 3D viewer renders your visual, enter rayshader::render_snapshot() into your console to take a flat snapshot of your 3D rendering, saving your snapshot if desired.\n\nbase_map %&gt;% \n  rayshader::add_overlay(polygon_layer) %&gt;%\n  rayshader::add_overlay(trails_layer) %&gt;%\n  rayshader::plot_3d(heightmap = laguna_zoom_mat, \n                     zscale = 0.75, fov = 0, \n                     theta = -45, zoom = 0.75, phi = 45,\n                     water = TRUE, waterdepth = 4, \n                     wateralpha = 0.4, watercolor = \"lightblue\",\n                     waterlinecolor = \"white\", waterlinealpha = 0.5,\n                     background = \"white\")\n\n# Create 2D snapshot of 3D rendering\nrayshader::render_snapshot()\n\n\n\n\n3D render of Laguna Beach"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#next-steps",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#next-steps",
    "title": "Beautiful Maps with Rayshader",
    "section": "Next Steps",
    "text": "Next Steps\nI’ve seen some neat examples where developers add 3D polygons to their maps to represent residential housing, commercial properties, and public facilities. I recently read about methods for rendering much higher-resolution maps (this would be great for printing). Creating 3D flyovers with rayshader also looks surprisingly high-end for open-source software. These are all things I hope to revisit at some point in the near future. TMW knocked it out of the park with his package and tutorials, so I hope you find his content as useful as I did."
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#session-info",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#session-info",
    "title": "Beautiful Maps with Rayshader",
    "section": "Session Info",
    "text": "Session Info\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.3.5    dplyr_1.0.7      sf_1.0-2         osmdata_0.1.6   \n[5] raster_3.4-13    sp_1.4-5         rayshader_0.26.1\n\nloaded via a namespace (and not attached):\n [1] rgl_0.107.14       Rcpp_1.0.7         lubridate_1.7.10   lattice_0.20-44   \n [5] png_0.1-7          prettyunits_1.1.1  class_7.3-19       ps_1.6.0          \n [9] assertthat_0.2.1   rprojroot_2.0.2    digest_0.6.27      foreach_1.5.1     \n[13] utf8_1.2.2         R6_2.5.1           e1071_1.7-8        httr_1.4.2        \n[17] pillar_1.6.2       rlang_0.4.11       progress_1.2.2     curl_4.3.2        \n[21] callr_3.7.0        magick_2.7.3       pkgdown_1.6.1      desc_1.3.0        \n[25] devtools_2.4.2     rgdal_1.5-23       htmlwidgets_1.5.3  munsell_0.5.0     \n[29] proxy_0.4-26       compiler_4.1.1     xfun_0.25          pkgconfig_2.0.3   \n[33] pkgbuild_1.2.0     htmltools_0.5.2    tidyselect_1.1.1   tibble_3.1.4      \n[37] codetools_0.2-18   fansi_0.5.0        crayon_1.4.1       withr_2.4.2       \n[41] grid_4.1.1         gtable_0.3.0       jsonlite_1.7.2     lifecycle_1.0.0   \n[45] DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       units_0.7-2       \n[49] KernSmooth_2.23-20 cli_3.0.1          cachem_1.0.6       farver_2.1.0      \n[53] fs_1.5.0           remotes_2.4.0      doParallel_1.0.16  testthat_3.0.4    \n[57] xml2_1.3.2         ellipsis_0.3.2     generics_0.1.0     vctrs_0.3.8       \n[61] iterators_1.0.13   tools_4.1.1        rayimage_0.6.2     glue_1.4.2        \n[65] purrr_0.3.4        hms_1.1.0          processx_3.5.2     pkgload_1.2.1     \n[69] parallel_4.1.1     fastmap_1.1.0      colorspace_2.0-2   sessioninfo_1.1.1 \n[73] classInt_0.4-3     rvest_1.0.1        memoise_2.0.0      knitr_1.33        \n[77] usethis_2.0.1"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#resources",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#resources",
    "title": "Beautiful Maps with Rayshader",
    "section": "Resources",
    "text": "Resources\n\nrayshader: https://www.rayshader.com/\nrayrender: http://www.rayrender.net/\nTyler Morgan-Wall, PhD: https://www.tylermw.com/"
  },
  {
    "objectID": "posts/2020-09-13-R-Package-Detailed-Tutorial/2020-09-13-R-Package-Detailed-Tutorial.html",
    "href": "posts/2020-09-13-R-Package-Detailed-Tutorial/2020-09-13-R-Package-Detailed-Tutorial.html",
    "title": "Detailed R Package Development Tutorial from Method Bites",
    "section": "",
    "text": "I’ll keep this short since all credit should go towards the amazing Cosima Meyer and Dennis Hammerschmidton… They compressed some of the most valuable steps for creating and deploying a package. This duo’s work was published for the MZES Social Science Data Lab. If you want a detailed and easy-to-follow how-to for R package development, bookmark this step-by-step guide!\n\nSource: * Methods Bites: How to write your own R package and publish it on CRAN\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-11-05-R-Shiny-Expert-Course/2019-11-05-R-Shiny-Expert-Course.html",
    "href": "posts/2019-11-05-R-Shiny-Expert-Course/2019-11-05-R-Shiny-Expert-Course.html",
    "title": "Shiny Developer Expert Course",
    "section": "",
    "text": "I am super excited about the new R Shiny Developer course that Matt Dancho launched, especially since he is the next guest on Scatter Podcast.\nHe’s one of the best data science educators that I’ve learned from and the deeper that I dive into my data science career, the more and more value I find from his Business Science courses, Learning Labs, his online tutorials (on the tidyverse, predictive analytics, data science best practices), his R packages… The guy is a beast!\nIf you are familiar with Shiny but want to gain some expert level Shiny skills, this class is for you. Great job with everything that you’re doing Matt and I’m excited to release your episode in a few days!\nLinks:\n\nBusiness Science’s Expert Shiny Developer with AWS\n15% off Business Science 4-course R bundle for Scatter Podcast listeners\nScatter Podcast Episode 25 w/ Matt Dancho\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2023-01-01-data-science-hangout/2023-01-01-data-science-hangout.html",
    "href": "posts/2023-01-01-data-science-hangout/2023-01-01-data-science-hangout.html",
    "title": "Data Science Hangout",
    "section": "",
    "text": "Background\nI’ve been attending Posit’s weekly Data Science Hangout since 2021 and it’s been one of my favorite standing weekly meetings. This is a great platform to hear how data science leaders are using R to drive business results and the host, Rachael Dempsey, does an excellent job fostering an inclusive and open culture. I was thrilled to be the guest several weeks ago in Dec 2022. 🚀\nI talked about my transition to data science from a career in financial modeling and consulting, talked about my love for Shiny and the R community, and gave some tips on how to start your own data science community. I also said “you know” an inordinate number of times, please forgive me in advance. 🙃\nWhether you’re a data science practitioner, student, looking to make a career pivot, or simply wanting to network, the Data Science Hangout has something for all. If you haven’t already, I highly encourage you to attend Posit’s Data Science Hangout live sometime in the future.\n\n\nWatch on YouTube\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html",
    "href": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html",
    "title": "R Goodies for 2023",
    "section": "",
    "text": "Data science had an awesome year in 2022. The list below highlights some of my favorites that I hope you test drive in 2023 (if you haven’t already!). Happy belated New Year folks!"
  },
  {
    "objectID": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#size-on-disk",
    "href": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#size-on-disk",
    "title": "R Goodies for 2023",
    "section": "Size on Disk",
    "text": "Size on Disk"
  },
  {
    "objectID": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#query-performance",
    "href": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#query-performance",
    "title": "R Goodies for 2023",
    "section": "Query Performance",
    "text": "Query Performance"
  },
  {
    "objectID": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#size-vs-performance",
    "href": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#size-vs-performance",
    "title": "R Goodies for 2023",
    "section": "Size vs Performance",
    "text": "Size vs Performance"
  },
  {
    "objectID": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#intro-to-sodium",
    "href": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#intro-to-sodium",
    "title": "R Goodies for 2023",
    "section": "Intro to Sodium",
    "text": "Intro to Sodium\nThe following example uses {sodium} to generate a keypair and encrpt / decrypt a serialized message (representing the iris data set). One major benefit of encrypting objects like this is that you can publicly expose these encrypted objects since you alone hold the key to decrypt them. If you’re working with GitHub Actions, GitLab CI/CD, or another git-backed repository, you can save these encrypted messages in your public repo - If someone forks or clones your repository, they’ll be unable to decrypt the object without the hash key initially created in the keypair generation step.\nThe example below was written by Jeroen Ooms on rOpenSci and is part of the documentation for the {sodium} package. I added the last step to show how to unserialize() an object previously decrypted.\n\nlibrary(sodium)\n\n# Generate keypair:\nkey &lt;- sodium::keygen()\npub &lt;- sodium::pubkey(key)\n\n# Encrypt message with pubkey\nmsg &lt;- serialize(iris, NULL)\nciphertext &lt;- sodium::simple_encrypt(msg, pub)\n\n# Decrypt message with private key\nout &lt;- sodium::simple_decrypt(ciphertext, key)\n\n# Unserialize the decrypted message & assign iris data to new obj\niris_new &lt;- unserialize(out)"
  },
  {
    "objectID": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#sodium-cyphr",
    "href": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#sodium-cyphr",
    "title": "R Goodies for 2023",
    "section": "Sodium + Cyphr",
    "text": "Sodium + Cyphr\nUsing the {sodium} and {cyphr} packages, the code snippets below showcase three key steps: 3a) data encryption, 3b) in-line decryption (to be used as a script intended to run with a GitHub Actions Workflow every six hours), and 3c) the GitHub Actions YAML file to configure the automation workflow.\nUsing this approach, I can safely expose an encrypted RDS file on my public GitHub repo that contains secret API credentials for accessing Meetup.com and fetching new events that may have been posted. The encrypted RDS file can only be decrypted using the keypair hash saved with the MEETUP_PWD environment variable on my GitHub repo, a secret env var only visible to me.\n\n\n\n\n\n\nGitHub Actions\n\n\n\nTo learn how to use secrets for use with GitHub Actions, the official Actions/Encrypted Secrets documentation is an excellent end-to-end resource. A generalized overview of how to configure your GitHub Actions Workflow is available on GitHub’s Actions/Workflow Syntax documentation. A brief overview of major updates to GitHub Actions for R Users was published on the Tidyverse Blog in June 2022, with the most complete resource for R related GitHub Actions being on r-lib/actions.\n\n\nNOTE: For the purposes of GitHub Actions and the config YAML file covered in Step 3c, this file lives in the following path on my GitHub repo: ~/.github/workflows/main.yml"
  },
  {
    "objectID": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#step-3a-encrypt",
    "href": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#step-3a-encrypt",
    "title": "R Goodies for 2023",
    "section": "Step 3a: Encrypt",
    "text": "Step 3a: Encrypt\n\n# The following is a one-time key encryption setup\n\nlibrary(sodium)\nlibrary(cyphr)\nlibrary(meetupr)\n\n# After doing a one-time interactive auth with {meetupr},\n# your Meetup token will be stored as an RDS file in the \n# same path as your {meetupr} package:\ntoken_path &lt;- \n  path.expand(\n    file.path(\"~\", \"Library\", \"Application Support\", \n              \"meetupr\", \"meetup_token.rds\")\n  )\n\nmeetupr::meetup_auth(\n  token = NULL,\n  cache = TRUE,\n  use_appdir = FALSE,\n  token_path = token_path\n)\n\n# Encrypt the existing token to safely save \n# meetupr_secret.rds in a public repo\nsodium_key &lt;- sodium::keygen()\n\n# In your working environment, save a secret env var called\n# \"MEETUPR_PWD\" using the randomly generated sodium_key\nSys.setenv(\"MEETUPR_PWD\" = sodium::bin2hex(sodium_key))\n\n# Create a key for use with {cyphr}\nkey &lt;- cyphr::key_sodium(sodium::hex2bin(Sys.getenv(\"MEETUPR_PWD\")))\n\n# Encrypt the RDS and save as meetupr_secret.rds\n# in the working environment\ncyphr::encrypt_file(\n  token_path,\n  key = key,\n  dest = \"meetupr_secret.rds\"\n)"
  },
  {
    "objectID": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#step-3b-decrypt",
    "href": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#step-3b-decrypt",
    "title": "R Goodies for 2023",
    "section": "Step 3b: Decrypt",
    "text": "Step 3b: Decrypt\n\n# This script was saved as \"meetup_events.R\" and it is run as part\n# of the GitHub Actions workflow on line 37 of the YAML file covered\n# in \"Step 3c: GitHub Actions YAML\"\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(purrr)\nlibrary(meetupr)\nlibrary(cyphr)\n\n# Generate Key with MEETUPR_PWD env var for MeetupR data CI/CD\nkey &lt;- cyphr::key_sodium(sodium::hex2bin(Sys.getenv(\"MEETUPR_PWD\")))\n\ntemptoken &lt;- tempfile(fileext = \".rds\")\n\ncyphr::decrypt_file(\"meetupr_secret.rds\",\n                    key = key,\n                    dest = temptoken)\n\ntoken &lt;- readRDS(temptoken)[[1]]\n\n# Non-Interactive Meetup Authorization\ntoken &lt;- meetupr::meetup_auth(\n  token = token,\n  use_appdir = FALSE,\n  cache = FALSE\n)\n\n# Save a temporary, updated Meetup personal access token that will\n# be used downstream with the meetupr::get_events() function\nSys.setenv(MEETUPR_PAT = temptoken)\n\n# Create a character vector of the Meetup accounts for data collection\nsocal_groups &lt;- c(\"SOCAL-RUG\", \n                  \"Santa-Barbara-R-Users-Group\", \n                  \"Real-Data-Science-USA-R-Meetup\",\n                  \"useR-Group-in-San-Luis-Obispo-County\", \n                  \"rladies-irvine\",\n                  \"rladies-la\",\n                  \"rladies-pasadena\",\n                  \"rladies-riverside\",\n                  \"rladies-santa-barbara\",\n                  \"rladies-san-diego\")\n\n# Create a function for getting upcoming and past Meetup events\nget_meetup_events &lt;- function(x) {\n  if (length(meetupr::get_events(urlname = x)) &gt; 0) {\n    meetupr::get_events(urlname = x) |&gt; \n      dplyr::mutate(chapter = x)\n  }\n}\n\n# Using purrr, iterate through each Meetup account, collect \n# event data, and row-bind the resulting event listings\nevents_raw &lt;- \n  purrr::map_dfr(.x = socal_groups, .f = get_meetup_events) |&gt; \n  dplyr::select(chapter, dplyr::everything())\n\n# Data clean up\nevents &lt;- \n  events_raw |&gt; \n  dplyr::filter(\n    status != \"draft\",\n    !stringr::str_detect(title, \"Cross-post|cross-post\"),\n    !stringr::str_detect(description, \"Cross-post|cross-post\")) |&gt; \n  dplyr::distinct() |&gt; \n  dplyr::arrange(dplyr::desc(time)) |&gt;\n  dplyr::mutate(\n    Upcoming = dplyr::if_else(status == \"published\", \"&#x2713;\", \"\"),\n    Chapter = dplyr::case_when(\n      chapter == \"SOCAL-RUG\" ~ \"SoCal RUG\",\n      chapter == \"Santa-Barbara-R-Users-Group\" ~ \"Santa Barbara RUG\",\n      chapter == \"Real-Data-Science-USA-R-Meetup\" ~ \"Los Angeles RUG\",\n      chapter == \"useR-Group-in-San-Luis-Obispo-County\" ~ \"SLO RUG\",\n      chapter == \"rladies-irvine\" ~ \"R-Ladies Irvine\",\n      chapter == \"rladies-la\" ~ \"R-Ladies Los Angeles\",\n      chapter == \"rladies-pasadena\" ~ \"R-Ladies Pasadena\",\n      chapter == \"rladies-riverside\" ~ \"R-Ladies Riverside\",\n      chapter == \"rladies-santa-barbara\" ~ \"R-Ladies Santa Barbara\",\n      chapter == \"rladies-san-diego\" ~ \"R-Ladies San Diego\",\n      TRUE ~ chapter),\n    Event = paste0('&lt;a href=', '\"', link, '\"&gt;', title, '&lt;/a&gt;'),\n    Date = lubridate::as_date(time),\n    Time = format.POSIXct(time, format = \"%I:%M %p\")) |&gt; \n  dplyr::select(Upcoming, Chapter, Event, Date, \"Time (PST)\" = Time)\n\nsaveRDS(events, file = \"events/events_past_and_upcoming.rds\")"
  },
  {
    "objectID": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#step-3c-github-actions-yaml",
    "href": "posts/2023-01-15-r-goodies-for-2023/2023-01-15-r-goodies-for-2023.html#step-3c-github-actions-yaml",
    "title": "R Goodies for 2023",
    "section": "Step 3c: GitHub Actions YAML",
    "text": "Step 3c: GitHub Actions YAML\n\nname: Render and Deploy Quarto Blog\non:\n  pull_request:\n    branches: [ master ]\n  push:\n    branches: [ master ]\n  schedule:\n    - cron:  '0 */6 * * *'\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout Repo\n        uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n          \n      - name: Install libsodium\n        run: sudo apt-get install -y libsodium-dev\n\n      - name: Setup Quarto CLI\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          version: 1.2.280\n\n      - name: Setup R \n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: '4.2.2'\n          use-public-rspm: true\n      \n      - name: Setup renv\n        uses: r-lib/actions/setup-renv@v2\n\n      - name: Fetch Events from Meetup\n        run: source(here::here(\"helpers\", \"meetup_events.R\"))\n        env:\n          MEETUPR_PWD: ${{ secrets.MEETUPR_PWD }}\n        shell: Rscript {0}\n\n      - name: Render and Publish to Netlify\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: netlify\n          NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}"
  },
  {
    "objectID": "posts/2023-03-28-shiny-bslib-socal-rug/2023-03-28-shiny-bslib-socal-rug.html",
    "href": "posts/2023-03-28-shiny-bslib-socal-rug/2023-03-28-shiny-bslib-socal-rug.html",
    "title": "Impressing Hiring Teams with a Shiny App Demo",
    "section": "",
    "text": "The Southern California R Users Group (“SoCal RUG”) is one of my favorite organizations - Not only is there a lot to be learned from regularly attending our events, it’s a great place to meet and network with other passionate data scientists in the community.\nLast week, I presented to SoCal RUG on how to build a Shiny app demo as a resume or cover letter accessory, an extension to the Build a Shiny App Demo blog post that I wrote in Nov 2022. Empowering yourself with this kind of ‘accessory’ ready to be demoed at your interviews will help impress data science hiring managers and also differentiate you from other applicants."
  },
  {
    "objectID": "posts/2023-03-28-shiny-bslib-socal-rug/2023-03-28-shiny-bslib-socal-rug.html#beginner-resources",
    "href": "posts/2023-03-28-shiny-bslib-socal-rug/2023-03-28-shiny-bslib-socal-rug.html#beginner-resources",
    "title": "Impressing Hiring Teams with a Shiny App Demo",
    "section": "Beginner Resources",
    "text": "Beginner Resources\n\nOfficial Shiny website: shiny.rstudio.com\n\nIncludes example galleries for inspiration\n\nDeploying your Shiny app to shinyapps.io: shiny.rstudio.com/articles/shinyapps.html\nShiny extension packages:\n\n{bs4Dash}: How to start? (step-by-step)\n\nBrings Bootstrap 4 support to Shiny themes\nRelies on AdminLTE HTML template\n\n{bslib}: Customizing “stock” Shiny apps\n\nGlobally style your Shiny Bootstrap themes\nUse Sass variables to further customize your apps with “Sassy CSS” (*.scss)"
  },
  {
    "objectID": "posts/2023-03-28-shiny-bslib-socal-rug/2023-03-28-shiny-bslib-socal-rug.html#advanced-resources",
    "href": "posts/2023-03-28-shiny-bslib-socal-rug/2023-03-28-shiny-bslib-socal-rug.html#advanced-resources",
    "title": "Impressing Hiring Teams with a Shiny App Demo",
    "section": "Advanced Resources",
    "text": "Advanced Resources\n\nHadley Wickham’s Mastering Shiny: mastering-shiny.org/\n{golem} for modularizing and packaging Shiny apps: thinkr-open.github.io/golem/index.html\nEngineering Production-Grade Shiny Apps: engineering-shiny.org/\nOutstanding User Interfaces with Shiny: unleash-shiny.rinterface.com/\nAutomating Dockerfile creation for Shiny apps: www.jumpingrivers.com/blog/shiny-auto-docker"
  },
  {
    "objectID": "posts/2023-03-28-shiny-bslib-socal-rug/2023-03-28-shiny-bslib-socal-rug.html#shiny-inspiration",
    "href": "posts/2023-03-28-shiny-bslib-socal-rug/2023-03-28-shiny-bslib-socal-rug.html#shiny-inspiration",
    "title": "Impressing Hiring Teams with a Shiny App Demo",
    "section": "Shiny Inspiration",
    "text": "Shiny Inspiration\n\nPosit’s Shiny gallery: shiny.rstudio.com/gallery/\nAppsilon’s Shiny demos: demo.appsilon.com/\nShiny Contest submissions by Stefan Schliebs:\n\n2021: New Zealand Commute Explorer\n2020: R Blog Explorer\n2019: R Package Explorer"
  },
  {
    "objectID": "posts/2020-04-05-DnD-Adventures/2020-04-05-DnD-Adventures.html",
    "href": "posts/2020-04-05-DnD-Adventures/2020-04-05-DnD-Adventures.html",
    "title": "D&D Adventures + blogdown",
    "section": "",
    "text": "Wizards of the Coast have developed such an incredible tabletop, fantasy, RPG with the 5th edition of D&D… I wish I would have re-discovered this game years back. For context, I haven’t touched D&D since 1997!\nIf you know of good resources for newbies and new D&D Dungeon Masters (“DMs”), I’d love to know about your favorite resources. For data science folks out there, this site was built with R using blogdown, Hugo themes, and deployed for free using GitHub and Netlify.\n\n\n\nD&D dragon looking for trouble\n\n\nReference:  * Javier’s D&D Adventures\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2021-10-21-torch-for-r/2021-10-21-torch-for-r.html",
    "href": "posts/2021-10-21-torch-for-r/2021-10-21-torch-for-r.html",
    "title": "Torch for R + luz",
    "section": "",
    "text": "Torch for R… wow! 🔥🔥🔥 I was recently having a discussion with a coworker about the benefits of Torch, especially the power of training one global model capable of hierarchical projections (awesome for time series) and predicting multiple group-specific regressions. I went down a Googling rabbit hole last weekend and came across some amazing articles by Sigrid Keydana (see links below) introducing torch to the R community and also recently releasing luz, a high-level R interface to Torch (“luz is to torch what Keras is to TensorFlow”).\nRStudio’s MLVerse team is doing really exciting things for the R machine learning and AI community. With torch, I no longer need to launch a conda environment for complex NNs (although having Python on your system is always handy 😅). And even better, “torch for R is built directly on top of libtorch, a C++ library that provides the tensor-computation and automatic-differentiation capabilities essential to building neural networks.” If you’re looking for fast NNs and deep learning solutions within the #rstats framework, give these packages a try. Happy Friday and happy learning! 🤓📚"
  },
  {
    "objectID": "posts/2021-10-21-torch-for-r/2021-10-21-torch-for-r.html#sources",
    "href": "posts/2021-10-21-torch-for-r/2021-10-21-torch-for-r.html#sources",
    "title": "Torch for R + luz",
    "section": "Sources",
    "text": "Sources\n\nTorch for R: https://torch.mlverse.org/\nIntro to Torch: Torch for R\nIntro to Luz: Que haja luz: More light for torch!\nIntro to TabNet: TabNet for Tabular Data with Torch\nIntro to Time Series with Torch: Introductory Time-Series Forecasting with Torch\nSigrid Keydana: Sigrid on LinkedIn"
  },
  {
    "objectID": "posts/2019-12-31-Ubuntu-Update/2019-12-31-Ubuntu-Update.html",
    "href": "posts/2019-12-31-Ubuntu-Update/2019-12-31-Ubuntu-Update.html",
    "title": "Ubuntu Update to 19.10",
    "section": "",
    "text": "Update Ubuntu 19.04 to 19.10 on dual-booted systems. Not sure exactly where things broke down for me, but as of right now, #Ubuntu seems inaccessible. Not too bummed about it as everything is backed up, but if you’ve recently had the same heartburn and found a solution, please send me a message!\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-01-04-CCPA/2020-01-04-CCPA.html",
    "href": "posts/2020-01-04-CCPA/2020-01-04-CCPA.html",
    "title": "California Consumer Privacy Act",
    "section": "",
    "text": "The California Consumer Privacy Act (“CCPA”) data rights include, but are not limited to, the following:\n\nthe right to know,\nthe right to delete,\nthe right to opt-out of personal data sale, and\nthe right to non-discrimination when a consumer exercises a privacy right under CCPA.\n\nCCPA will only apply to businesses that meet one or more of these three criteria:\n\nhave annual revenues greater than $25 million USD,\nbusinesses that buy / receive / sell personal information of 50k+ consumers, or\nbusinesses that derive 50% or more of annual revenues from selling consumers’ personal information\n\nCCPA exempt businesses include HIPAA-compliant health insurers and providers, certain financial institutions, and credit reporting agencies.\nThe expected fine per unintentional and intentional violation is $2,500 and $7,500, respectively. Fines make sense, but I hope that the California Department of Justice can establish clear reporting and audit requirements to enforce these new regulations.\nSource:\n\nCNET’s oveview of CCPA\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-02-20-Themis/2020-02-20-Themis.html",
    "href": "posts/2020-02-20-Themis/2020-02-20-Themis.html",
    "title": "themis: Extra Steps for tidymodels + recipes",
    "section": "",
    "text": "Working with unbalanced data sets? Remember that accuracy, alone, is not the best performance metric (especially when dealing with unbalanced data). Instead, place more importance on Cohen’s kappa coefficient, F1 harmonic mean, or focus on improving your model’s specificity or sensitivity, etc.\nI’ve been transitioning a lot of my workflows to the tidymodels framework and I am super excited about the future of tidymodels (recipes + parsnip + dials + tune + workflow + more 😭✊🙌). If you’re using recipes often like me, a new library called {themis}, by Emil Hvitfeldt expands the {recipes} pre-processing steps for working with unbalanced data sets (it adds functionality for under- and hybrid-sampling techniques). I love me some smote, and now I can incorporate this sampling technique into my recipes with themis::step_smote()!\n# Installation\ninstall.packages(\"themis\")\n\n# Example workflow\nlibrary(recipes)\nlibrary(modeldata)\nlibrary(themis)\n\ndata(okc)\n\nsort(table(okc$Class, useNA = \"always\"))\n#&gt; \n#&gt;  &lt;NA&gt;  stem other \n#&gt;     0  9539 50316\n\nds_rec &lt;- recipe(Class ~ age + height, data = okc) %&gt;%\n  step_meanimpute(all_predictors()) %&gt;%\n  step_smote(Class) %&gt;%\n  prep()\n\nsort(table(bake(ds_rec, new_data = NULL)$Class, useNA = \"always\"))\n#&gt; \n#&gt;  &lt;NA&gt;  stem other \n#&gt;     0 50316 50316\nSource: * themis * themis on GitHub\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-01-22-Systematically-Improving-Coffee/2020-01-22-Systematically-Improving-Coffee.html",
    "href": "posts/2020-01-22-Systematically-Improving-Coffee/2020-01-22-Systematically-Improving-Coffee.html",
    "title": "An Algorithm for Better Espresso",
    "section": "",
    "text": "I upgraded my espresso machine over the holidays and have been pleasantly surprised by the need to recalibrate my machine’s settings with every new batch of coffee (and of course, fresh roasted is best). Glad to see some math support my observations. :-)\n\nSource:\n\nSystematically Improving Espresso: Insights from Mathematical Modeling and Experiment\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-12-04-Finland-Education-System/2019-12-04-Finland-Education-System.html",
    "href": "posts/2019-12-04-Finland-Education-System/2019-12-04-Finland-Education-System.html",
    "title": "Education Reform and Lessons from Finland",
    "section": "",
    "text": "Despite Finnish students spending the least time per week learning (and starting school later, and having much less homework), their students have some of the highest reading test scores.\nNot captured in this study, but I’d be curious to see how external factors impact these results. We have to assume American parents spend a lot less time helping their kids learn (imagine having parents that work 2 or 3 jobs to make ends meet). Or how does average classroom size differ from country to country? Or distance traveled from home to school? Or distance to local libraries? Either way, the Finnish are doing something right here.\nLinks:\n\nFinland has the most efficient education system in the world on Quartz\nGoogle’s Avinash Kaushik’s post on LinkedIn\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-10-19-Information-is-Beautiful-Awards/2019-10-19-Information-is-Beautiful-Awards.html",
    "href": "posts/2019-10-19-Information-is-Beautiful-Awards/2019-10-19-Information-is-Beautiful-Awards.html",
    "title": "Information is Beautiful 2019 Awards",
    "section": "",
    "text": "A few weeks ago, Will Chase was a guest on Scatter Podcast to talk about his work as a researcher and data visualization professional.\nSuper exciting news for Will… His visualizing earthquake risk project was shortlisted for the Information is Beautiful 2019 Awards (the Science & Technology category). Give it a look and if you like what you see, please vote for his amazing project!! Congrats Will!\nLinks:\n\nVote for Will’s project on Kantar’s Information is Beautiful 2019 Awards\nWill’s (new!) website: www.williamrchase.com\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-11-11-R-Markown/2019-11-11-R-Markown.html",
    "href": "posts/2019-11-11-R-Markown/2019-11-11-R-Markown.html",
    "title": "15 Tips for Making Better Use of R",
    "section": "",
    "text": "It’s such a great way to make reports come to life. I’ve been using it more in the last month than I have in the last year and can’t believe how much more interactive I’ve made my deliverables (and even my own data exploration). Really excited to see what new tricks I can learn from RStudio’s webinar on Friday, November 15!\nLinks:\n\nRStudio Webinar Signup on Eventbrite\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-11-04-dbplyr-2.0.0/2020-11-04-dbplyr-2.0.0.html",
    "href": "posts/2020-11-04-dbplyr-2.0.0/2020-11-04-dbplyr-2.0.0.html",
    "title": "What’s New in dbplyr 2.0.0",
    "section": "",
    "text": "dbplyr 2.0.0 comes loaded with improvements including the translation of dplyr’s new across() function. If you aren’t familiar with this library, dbplyr translates your dplyr syntax to SQL. I regularly connect to enterprise databases using R and have almost completely transitioned from SQL. Yes, SQL is awesome. Yes, using dplyr is a lot more fun!\n\nSource: * tidyverse blog’s dbplyr 2.0.0 Official Announcement\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-10-22-BBC-Visualization-Journey-with-R/2019-10-22-BBC-Visualization-Journey-with-R.html",
    "href": "posts/2019-10-22-BBC-Visualization-Journey-with-R/2019-10-22-BBC-Visualization-Journey-with-R.html",
    "title": "Visual & Data Journalism at BBC",
    "section": "",
    "text": "Great read, AND super neat to see the BBC team open-source their BBPLOT and R Cookbook to help streamline your visualization workflows, reduce manual repetition of code setup for a ggplot2 viz, and to educate you on better storytelling approaches with R.\nLinks:\n\n“How the BBC Visual and Data Journalism team works with graphics in R” by BBC Visual and Data Journalism\nBBC’s bbplot package on GitHub\nBBC’s R Cookbook\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-04-19-Police-Killings/2020-04-19-Police-Killings.html",
    "href": "posts/2020-04-19-Police-Killings/2020-04-19-Police-Killings.html",
    "title": "Lying with Statistics",
    "section": "",
    "text": "Lets explore four plots and see how we can #LieWithStatistics…\nPlot 1: Police killings by date, by race General observation: Police kill more white people than black people \nPlot 2: Police killing boxplot showing murder rates, by race, by police department General explanation and takeaway: The dots on each boxplot show the statistical outliers, box plot lines extend out to the “min” and “max”, and the box lines (from bottom to top of each box) represent the first quartile (25th percentile), median (50th percentile), and third quartile (75th percentile) \nPlot 3: Police killing boxplot, now log-transforming the murder rates to more easily identify statistical differences, by race General explanation and takeaway: Log-transforming data points for visualization or modeling purposes is a technique by which you can smooth observed data making it more robust (or resistant) to outliers. I effectively re-wrote the murder rates to show exponential relativity. Important caveat: Are Native Americans more likely to die by police than other races? Sure looks like it… but see Plot 4 for more thoughts \nPlot 4: Police killing boxplot, now log-transforming the murder rates using a log base 10 (easier interpretability) and “fixing” the Native American data points causing a misleading assumption in Plot 3, i.e., Native American death rates appeared much higher than others in Plot 3 given the fact that log(0) = 1. General takeaway: There were such few Native American data points that log-transforming all of the zeroes was unintentionally bastardizing the analysis. It would appear black people are almost an order of magnitude more likely to be killed by police than white people. \nI do not seek to answer questions of “why” systemic injustice exists in the US, but I wanted to analyze police killing data and share these dialectical investigations.\nSource: * Samuel Sinyangwe & the Mapping Police Violence team\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-06-20-dplyr-1.0.0/2020-06-20-dplyr-1.0.0.html",
    "href": "posts/2020-06-20-dplyr-1.0.0/2020-06-20-dplyr-1.0.0.html",
    "title": "Testing dplyr 1.0.0",
    "section": "",
    "text": "I had read about the new across() function hype and now I get it… No more mutate_at() or summarise_at() with the cryptic list-lambdas. As the dplyr documentation states, “across() supersedes the family of”scoped variants” like summarise_at(), summarise_if(), and summarise_all().”\nBelow is an example highlighting the new across() syntax meant to be used within a mutate() function. I tried this on a few columns of the mtcars data set with dplyr’s case_when() a la SQL CASE_WHEN (as opposed to nesting multiple if_else() statements to the point of confusion!). The dplyr changes are subtle but will definitely streamline my data wrangling.\n\n\n\ndplyr 1.0.0’s new across() function\n\n\nSource: * dplyr, a core tidyverse package\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html",
    "href": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html",
    "title": "rstudio::global tips, tricks, and more",
    "section": "",
    "text": "I attended several sessions throughout rstudio::global and below are my favorite reminders, takeaways, and new concepts that I learned. You’ll gain the most from this article if you are comfortable with predictive analytics with R and have been exposed to the tidyverse and tidymodels collections of packages. If you missed the conference, all of the presentations and slides will be made available on RStudio’s website soon. Some philosophical, some technical."
  },
  {
    "objectID": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html#irregular-grid-searches",
    "href": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html#irregular-grid-searches",
    "title": "rstudio::global tips, tricks, and more",
    "section": "Irregular Grid Searches",
    "text": "Irregular Grid Searches\nIrregular grid searches help you tune your models by cycling through randomized hyperparameters in an effort to yield the best (or at least better) performance. I’ve been using the dials package to create Latin hypercubes which is explained below, highlighting the benefits of this space-filling design vs a purely randomized irregular grid search. Max Kuhn’s “New in tidymodels” rstudio::global event provided me with a fresh reminder about why these methods are beneficial and how they actually optimize performance.\nThe documentation below was copied almost entirely from Chapter 13 of Max Kuhn and Julia Silge’s Tidy Modeling with R (“TMWR”). I’ve deleted certain bits and added minimal helper comments in brackets. If you find yourself statistically programming, bootstrapping / resampling, improving model performance through grid searches, maybe even ensemble stacking, I’d highly recommend that you bookmark the TMWR book.\n\nRandom Grid Search\nThere a several options for creating non-regular grids. The first is to use random sampling across the range of parameters. The grid_random() function [of the dials package] generates independent uniform random numbers across the parameter ranges. If the parameter object has an associated transformation (such as we have for penalty), the random numbers are generated on the transformed scale. For example:\n\nset.seed(10)\nmlp_param %&gt;% \n  grid_random(size = 1000) %&gt;% # 'size' is the number of combinations\n  summary()\n#&gt;   hidden_units      penalty           epochs    \n#&gt;  Min.   : 1.00   Min.   :0.0000   Min.   :  10  \n#&gt;  1st Qu.: 3.00   1st Qu.:0.0000   1st Qu.: 259  \n#&gt;  Median : 6.00   Median :0.0000   Median : 480  \n#&gt;  Mean   : 5.58   Mean   :0.0432   Mean   : 496  \n#&gt;  3rd Qu.: 8.00   3rd Qu.:0.0050   3rd Qu.: 738  \n#&gt;  Max.   :10.00   Max.   :0.9932   Max.   :1000\n\nFor penalty, the random numbers are uniform on the log (base 10) scale but the values in the grid are in the natural units.\nThe issue with random grids is that, with small-to-medium grids, random values can result in overlapping parameter combinations. Also, the random grid needs to cover the whole parameter space but the likelihood of good coverage increases with the number of grid values. Even for a sample of 15 candidate points, this plot shows some overlap between points for our example multilayer perceptron:\n\nlibrary(ggforce)\nset.seed(200)\nmlp_param %&gt;% \n  # The 'original = FALSE' option keeps penalty in log10 units\n  grid_random(size = 15, original = FALSE) %&gt;% \n  ggplot(aes(x = .panel_x, y = .panel_y)) + \n  geom_point() +\n  geom_blank() +\n  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + \n  labs(title = \"Random design with 15 candidates\")\n\n\n\n\nRandom grid search using dials::grid_random()\n\n\n\n\nSpace-Filling Designs\nA much better approach is to use a set of experimental designs called space-filling designs. While different design methods have slightly different goals, they generally find a configuration of points that cover the parameter space with the smallest chance of overlapping or redundant values. See Santner et al. (2003) for an overview of space-filling designs.\nThe dials package contains functions for Latin hypercube and maximum entropy designs. As with dials::grid_random(), the primary inputs are the number of parameter combinations and a parameter object. Let’s compare the above random design with a Latin hypercube design for 15 candidate parameter values.\n\nset.seed(200)\nmlp_param %&gt;% \n  grid_latin_hypercube(size = 15, original = FALSE) %&gt;% \n  ggplot(aes(x = .panel_x, y = .panel_y)) + \n  geom_point() +\n  geom_blank() +\n  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + \n  labs(title = \"Latin Hypercube design with 15 candidates\")\n\n\n\n\nLatin hypercube designed using the dials::grid_latin_hypercube() helper\n\n\nWhile not perfect, this design spaces the points further away from one another.\nSpace-filling designs can be very effective at representing the parameter space. The default design used by the tune package is the maximum entropy design. These tend to produce grids that cover the candidate space well and drastically increase the chances of finding good results.\nTo learn more about advanced iterative search methods such as Bayesian optimization and simulated annealing, please visit Chapter 14 of TMWR. Max Kuhn covered these advanced techniques during his tidymodels rstudio::global event and I haven’t had a chance to try these iterative search methods."
  },
  {
    "objectID": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html#usemodels",
    "href": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html#usemodels",
    "title": "rstudio::global tips, tricks, and more",
    "section": "usemodels",
    "text": "usemodels\nIn the spirit of sharing useful tips, tricks, and packages, I’d like to introduce you to Max Kuhn’s usemodels package. If you’re on the fence about adopting the tidymodels framework, this package is definitely for you. Even for experienced users, usemodels is an excellent way to quickly generate boilerplate code snippets that are algorithm-specific. This package is not yet on CRAN, so install it using the following:\n\n# Installing usemodels from its GitHub repo\ndevtools::install_github(\"tidymodels/usemodels\")\n\nGiven a simple formula and a data set, the use_* functions can create code that is appropriate for the data (given the model). [Note: The model formula will be in the form of y ~ a + b + c or y ~ . if you plan on including all available variables in your model.]\nFor example, using the palmerpenguins data with a glmnet model:\n\nlibrary(usemodels)\nlibrary(palmerpenguins)\ndata(penguins)\nuse_glmnet(body_mass_g ~ ., data = penguins)\n\n# NOTE: The below will be printed in your console with your model recipe and tailored\n# with the required pre-processing steps given your algorithm of choice (in this case,\n# glmnet). The `usemodels` output also provides the code structure for a reproducible \n# workflow, made possible with the `workflows` package. Should you choose to tune\n# your model, `usemodels` also provides code snippets for producing a grid of parameter\n# combinations to use with your hyperparameter tuning grid search.\n\nglmnet_recipe &lt;- \n  recipe(formula = body_mass_g ~ ., data = penguins) %&gt;% \n  step_novel(all_nominal(), -all_outcomes()) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_predictors(), -all_nominal()) \n\nglmnet_spec &lt;- \n  linear_reg(penalty = tune(), mixture = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") \n\nglmnet_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(glmnet_recipe) %&gt;% \n  add_model(glmnet_spec) \n\nglmnet_grid &lt;- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), mixture = c(0.05, \n    0.2, 0.4, 0.6, 0.8, 1)) \n\nglmnet_tune &lt;- \n  tune_grid(glmnet_workflow, resamples = stop(\"add your rsample object\"), grid = glmnet_grid)\n\nAs of today’s date, this package includes templates available with the following code: use_cubist, use_earth, use_glmnet, use_kknn, use_ranger, and use_xgboost"
  },
  {
    "objectID": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html#stacks",
    "href": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html#stacks",
    "title": "rstudio::global tips, tricks, and more",
    "section": "stacks",
    "text": "stacks\n\nEnsemble Models\nSimilar to usemodels, this is a very early-stage package that you should start following. There are already several AI tools for producing “model stacks” or “ensemble models” such as H2O.ai, DataRobot, and a few others, however, stacks is the first purpose-built for use with tidymodels. The following narrative is copied almost directly from the stacks website.\nstacks is an R package for model stacking that aligns with the tidymodels. Model stacking is an ensembling method that takes the outputs of many models and combines them to generate a new model - referred to as an ensemble in this package - that generates predictions informed by each of its members.\nThe process goes something like this:\n\nDefine candidate ensemble members using functionality from rsample, parsnip, workflows, recipes, and tune\nInitialize a data_stack object with stacks()\nIteratively add candidate ensemble members to the data_stack with add_candidates()\nEvaluate how to combine their predictions with blend_predictions()\nFit candidate ensemble members with non-zero stacking coefficients with fit_members()\nPredict on new data with predict()\n\n\n\nstacks Grammar\nAt the highest level, ensembles are formed from model definitions. In this package, model definitions are an instance of a minimal workflow, containing a model specification (as defined in the parsnip package) and, optionally, a preprocessor (as defined in the recipes package). Model definitions specify the form of candidate ensemble members.\n\nTo be used in the same ensemble, each of these model definitions must share the same resample. This rsample rset object, when paired with the model definitions, can be used to generate the tuning/fitting results objects for the candidate ensemble members with tune.\n\nCandidate members first come together in a data_stack object through the add_candidates() function. Principally, these objects are just tibbles, where the first column gives the true outcome in the assessment set (the portion of the training set used for model validation), and the remaining columns give the predictions from each candidate ensemble member. (When the outcome is numeric, there’s only one column per candidate ensemble member. Classification requires as many columns per candidate as there are levels in the outcome variable.) They also bring along a few extra attributes to keep track of model definitions.\n\nThen, the data stack can be evaluated using blend_predictions() to determine to how best to combine the outputs from each of the candidate members. In the stacking literature, this process is commonly called meta-learning.\nThe outputs of each member are likely highly correlated. Thus, depending on the degree of regularization you choose, the coefficients for the inputs of (possibly) many of the members will zero out—their predictions will have no influence on the final output, and those terms will thus be thrown out.\n\nThese stacking coefficients determine which candidate ensemble members will become ensemble members. Candidates with non-zero stacking coefficients are then fitted on the whole training set, altogether making up a model_stack object.\n\nThis model stack object, outputted from fit_members(), is ready to predict on new data! The trained ensemble members are often referred to as base models in the stacking literature. To learn more about how to use stacks, check out the following excellent vignettes from the tidymodels team:\n\nGetting Started with stacks\nClassification Models with stacks\n\nSources:\n\nrstudio::global(2021)\nTidy Modeling with R\ntidymodels\nusemodels\nstacks"
  },
  {
    "objectID": "posts/2020-12-19-R-Package-One-Hour-Tutorial/2020-12-19-R-Package-One-Hour-Tutorial.html",
    "href": "posts/2020-12-19-R-Package-One-Hour-Tutorial/2020-12-19-R-Package-One-Hour-Tutorial.html",
    "title": "One-Hour R Package Development Tutorial by Shannon Pileggi, PhD",
    "section": "",
    "text": "Package development should be on every R programmer’s radar in 2021, especially with all the free information available online! Here’s an excellent, concise, ultra-bookmark-worthy tutorial by Shannon Pileggi, PhD, titled “Your first R package in 1 hour”. Folks, she wasn’t lying either. I followed her tutorial and built an R package in roughly 45 minutes (note: I assume you have experience with the R language and understand the basics of devtools, usethis, roxygen2, and rmarkdown).\nUsing functions from just the devtools and usethis packages, the whole process is much more streamlined than what I would have thought. Check out the rest of Shannon’s Piping Hot Data website as well… It’s a great resource for R programmers and researchers, and she built it with the newly reintroduced distill package, just like this site. :-)\nSource: * Shannon’s Piping Hot Data “Your first R package in 1 hour” post * Shannon’s LinkedIn\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2022-11-15-shiny-bslib-demo/2022-11-15-shiny-bslib-demo.html",
    "href": "posts/2022-11-15-shiny-bslib-demo/2022-11-15-shiny-bslib-demo.html",
    "title": "Build a Shiny App Demo",
    "section": "",
    "text": "If you Google Search “applying for jobs is,” you see auto-completion suggestions that feel about right…\nApplying for jobs is both exhausting and a full-time job. Data scientists are expected to stay on top of their statistical education, have robust GitHub repositories, and they’re expected to know how to apply ever-changing technologies for DevOps and reproducible machine learning… And all this in addition to the endless resume and cover letter tweaks you’re probably making for each job (as you should).\nIf you’re an academic wanting to pivot to industry or a data scientist looking for a job change, I propose a simple (or simple-ish) solution to standing out from the crowd in a meaningful way: Build an easy-to-access web application for your future employers.\nIn July 2022, Travis Gerke held a presentation at rstudio::conf(2022) titled What They Forgot to Teach You About Industry Transitions from Academia (or “WTF-AITA” 😬). Travis presented my Shiny app as a “cover letter accessory” and it motivated me to write this tutorial such that anybody familiar with R, GitHub, and the RStudio IDE could deploy their own demo Shiny app for prospective employers."
  },
  {
    "objectID": "posts/2022-11-15-shiny-bslib-demo/2022-11-15-shiny-bslib-demo.html#clone-repo",
    "href": "posts/2022-11-15-shiny-bslib-demo/2022-11-15-shiny-bslib-demo.html#clone-repo",
    "title": "Build a Shiny App Demo",
    "section": "1: Clone Repo",
    "text": "1: Clone Repo\nWe’ll start by forking this GitHub repo into your repository to allow you to experiment with my code without actually changing anything in my original project repo. To do this, navigate to my GitHub repo ( JavOrraca/bslib_demo_shiny) and click on the Fork button in the top-right of the repo. This will prompt you to “Create a new fork” - Typically, I like to keep all of the defaults intact. When you’re ready to create the new fork, simply hit the Create fork button on the bottom of the screen.\nAssuming you kept the original repository name, you should see a newly created repo titled “bslib_demo_shiny” in your GitHub repositories. It contains a full copy of all files and folders found in my original project repo.\nNext, you’ll want to clone this repo to your local machine. Using the RStudio IDE, navigate to its menu bar and click on File -&gt; New Project -&gt; Version Control -&gt; Git, copy/paste your repo’s clone URL, choose your local project directory path of choice, and click on Create Project.\nRStudio IDE’s New Project Wizard will launch in a popup window after selecting New Project from the file menu. You should expect the following:\n\n\n\n\n\nThis process will create a local clone of all files and folders found on your remote GitHub repo. Once all files are downloaded for this project, the files and folder structure will look like this:\n├── Dockerfile ├── LICENSE ├── README.md ├── bslib_demo_shiny.Rproj ├── global.R ├── helpers │ ├── custom_theme.R │ ├── footer.R │ └── navbar.R ├── modules │ ├── mod_About.R │ ├── mod_Mapping.R │ ├── mod_Overview.R │ └── mod_Reports.R ├── renv │ ├── activate.R │ ├── library │ │ └── R-4.2 │ ├── sandbox │ │ └── R-4.2 │ ├── settings.dcf │ └── staging ├── renv.lock ├── server.R ├── ui.R ├── www │ ├── Shiny_Demo_Preview.png │ ├── blr_logo-primary.png │ └── styles.scss"
  },
  {
    "objectID": "posts/2022-11-15-shiny-bslib-demo/2022-11-15-shiny-bslib-demo.html#install-packages",
    "href": "posts/2022-11-15-shiny-bslib-demo/2022-11-15-shiny-bslib-demo.html#install-packages",
    "title": "Build a Shiny App Demo",
    "section": "2: Install Packages",
    "text": "2: Install Packages\nIt’s important to make note of the renv folder. This Shiny app relies on the renv package to manage project-package dependencies. If the Shiny app requires different package versions than those installed on your system’s main R library path, this approach will install the necessary packages in the renv directory.\nTo install the required R packages for this app, follow the instructions presented in your RStudio IDE Console immediately after cloning the GitHub repo or execute the command renv::restore(). This process will restore the package state as recorded in the renv.lock file - This “lockfile” records all of the R package names and versions required to run this specific Shiny app.\nOnce the project-package dependencies are installed, test launch the Shiny app by running shiny::runApp() in the Console or by clicking on the Run App button in the RStudio IDE. The Run App button only appears if you’re inside the global.R, ui.R, and server.R files - These three filenames are unique to Shiny and the RStudio IDE recognizes them as such."
  },
  {
    "objectID": "posts/2022-11-15-shiny-bslib-demo/2022-11-15-shiny-bslib-demo.html#app-customization",
    "href": "posts/2022-11-15-shiny-bslib-demo/2022-11-15-shiny-bslib-demo.html#app-customization",
    "title": "Build a Shiny App Demo",
    "section": "3: App Customization",
    "text": "3: App Customization\nWhile there are many ways to configure a Shiny app, the global aesthetics of this app have been configured using the bslib and thematic packages. These packages provide tools for customizing Bootstrap themes and they introduce Sass variables to Shiny (in my opinion, making the customization process much more unified).\nThe default Bootstrap version for bslib is version 5, however, this app was designed using Bootstrap 4. The thematic package provides a centralized approach to styling certain R visualizations such as those generated from ggplot2.\nTo fully customize the color scheme and overall theme of the app to your liking, make revisions to the following files that are sourced at runtime:\n\nhelpers/custom_theme.R\nThis helper file creates two functions (fn_custom_theme() and fn_thematic_theme()) that are loaded into your global environment at runtime. Here you can specify base fonts, background (bg) and foreground (fg) colors, and other accent colors used throughout the Shiny app. The fn_custom_theme() function is called in line 3 of ui.R (in the app’s root directory) and the fn_thematic_theme() function is called in line 56 of modules/mod_Overview.R.\n\n# Overarching bslib theme\nfn_custom_theme &lt;- function() {\n  bslib::bs_theme(\n    version = \"4\",\n    base_font = sass::font_google(\"Open Sans\"),\n    bg = \"#ffffff\",\n    fg = \"#1d2d42\",\n    primary = \"#f3d436\", \n    secondary = \"#1d2d42\",\n    success = \"#1d2d42\") |&gt; \n    bs_add_variables(\"border-bottom-width\" = \"6px\",\n                     \"border-color\" = \"$primary\",\n                     .where = \"declarations\") |&gt; \n    bs_add_rules(sass::sass_file(\"www/styles.scss\"))\n}\n\n# Thematic theme for ggplot2 outputs\nfn_thematic_theme &lt;- function() {\n  thematic::thematic_theme(\n    bg = \"#ffffff\",\n    fg = \"#1d2d42\",\n    accent = \"#f3d436\",\n    font = font_spec(sass::font_google(\"Open Sans\"), scale = 1.75)\n  )\n}\n\n\n\nhelpers/footer.R\nThis file creates the fn_footer() helper function that powers the Shiny app’s footer. The fn_footer() function uses Shiny tags in conjunction with htmltools::HTML() (exported to the shiny package) to create an HTML footer tag that can process raw HTML. I’ve used it to include my name and website in the footer and the fn_footer() function is called in line 7 of ui.R.\n\nfn_footer &lt;- function(){\n  tags$footer(\n    HTML(\n      \"This web app demo was developed by &lt;a href='https://www.javierorracadeatcu.com'&gt;Javier Orraca-Deatcu&lt;/a&gt; with R + Shiny\"\n    )\n  )\n}\n\n\n\nhelpers/navbar.R\nThis script sets the navigation bar’s branding (logo and hyperlink) and wraps it into the helper function fn_navbar(). This function is passed to the title argument on line 5 of ui.R and it creates an HTML div with a custom class attribute (bloomreach_logo) that can be further styled with CSS in the www/styles.scss “Sassy CSS” file.\n\n\n\n\n\n\nAbout Shiny’s www folder\n\n\n\nThe image path in line 5, below, does not specify a full path despite this image existing in the www folder. In Shiny projects, the www directory is used for storing elements that will be rendered in the web browser and not from script outputs. We do not need to include the full path on line 5 of helpers/navbar.R as Shiny knows to look for blr_logo-primary.png in the www folder.\n\n\n\nfn_navbar &lt;- function(){\n  div(\n    class = \"bloomreach_logo\",\n    a(href = \"https://www.bloomreach.com/\",\n      img(src = \"blr_logo-primary.png\",\n          title = \"Bloomreach\")\n    )\n  )\n}\n\n\n\nmodules/About.R\nThis script defines the UI and Server functions used to render the About page on the Shiny app. Technically, the Overview, Mapping, Reports, and About pages of the Shiny app are tabs toggled with shiny::tabPanel(). Now that you’ve had some exposure to htmltools::HTML(), you should be able to revise the raw HTML in this script to build your own About page. It’s fairly straightforward so I won’t show this module in a code chunk - Please reach out to me directly or in the comments section if you have any questions."
  },
  {
    "objectID": "posts/2022-11-15-shiny-bslib-demo/2022-11-15-shiny-bslib-demo.html#example-tweaks",
    "href": "posts/2022-11-15-shiny-bslib-demo/2022-11-15-shiny-bslib-demo.html#example-tweaks",
    "title": "Build a Shiny App Demo",
    "section": "4: Example Tweaks",
    "text": "4: Example Tweaks\nNow that we’ve covered the files in your Shiny project that need to be updated if you want to deploy your own custom-styled demo Shiny app, let’s pivot to a real world example. Let’s say our name is Steve Wozniak and we want to design a Shiny app that invokes a retro Apple feel… After finding a decent logo to use and saving it to www/retro_apple_logo.png, below are the steps that we would need to follow to update our app:\n\nhelpers/custom_theme.R\n\nLine 5: base_font = sass::font_google(\"Press Start 2P\", local = FALSE, display = \"swap\"),\nLine 8: primary = \"#01c700\",\nLine 22: accent = \"#01c700\",\n\nhelpers/footer.R\n\nLine 4: \"This web app demo was developed by &lt;a href='http://www.woz.org/'&gt;Steve Wozniak&lt;/a&gt; with R + Shiny\"\n\nhelpers/navbar.R\n\nLine 3: class = \"retro_apple\",\nLine 4: a(href = \"http://www.apple.com/\",\nLine 5: img(src = \"retro_apple_logo.png\",\nLine 6: title = \"Apple\")\n\n\nLet’s give this app a test run and see how it changed the appearance:\n\n\n\n\n\nSo far, so good! We’ve got a retro logo (re-sized to fit in the top-left of the navbar), a retro font (pulled from Google Fonts), and the primary and accent colors have been updated from yellow to neon green. There are several additional edits we could make but for now, I like it."
  },
  {
    "objectID": "posts/2022-11-15-shiny-bslib-demo/2022-11-15-shiny-bslib-demo.html#app-deployment",
    "href": "posts/2022-11-15-shiny-bslib-demo/2022-11-15-shiny-bslib-demo.html#app-deployment",
    "title": "Build a Shiny App Demo",
    "section": "5: App Deployment",
    "text": "5: App Deployment\nNow we’re ready to publish your newly revised Shiny app. We’ll use the shinyapps.io platform by Posit to push-button deploy our app from the RStudio IDE. For our needs, a free account will suffice as the free tier allows up to 5 Shiny app deployments with a combined 25 monthly active hours of usage. It’s important to note that if someone launches your app and keeps it running in a tab, this is considered “active” runtime, so encourage your shinyapps.io users to close the browser window when they’re done using your app.\nIf you do not have a shinyapps.io account or you have not synced your RStudio IDE to shinyapps.io via security token, please review the shinyapps.io Getting Started documentation. Once you’re set up, it will be a breeze to publish your app. If you’re stuck, again feel free to reach out to me directly.\nSimilar to the way that the Run App button will present itself if you’re inside the global.R, ui.R, or server.R files, you’ll also see a blue sync-looking button that gives you the power of push-button deployment from within the RStudio IDE:\n Clicking that button will open a new window to help you publish your app to the shinyapps.io servers. The title you select will be the name of the application on your shinyapps.io dashboard and it will also be included in the URL of your app. For example, I named my app “Bloomreach_Shiny_App” so the URL to visit my app is https://javierorraca.shinyapps.io/Bloomreach_Shiny_App/.\n\n\n\n\n\nIf you use a title that already exists in your shinyapps.io dashboard, you will force overwrite the prior app with the same name, so take caution when publishing. More often than not, it’s desired to overwrite an app so that you can regularly push updates to production to the same URL, however, there are instances where this behavior would not be desired. Regardless of your publishing intention, RStudio IDE will provide a warning to confirm that you want to write over a shinyapps.io application with the same name.\nSince the Dockerfile is not relevant to this project deployment, I recommend un-checking Dockerfile from the list of file from which to publish. Once you’re ready, click on Publish and a new tab, Deploy, will be visible in the bottom-left of your RStudio IDE. This process takes several minutes since shinyapps.io will 1) install an R image with all the project-package dependencies needed to run your Shiny app, and 2) build and deploy the Shiny app itself. Once publishing is complete, you should see something like this:"
  },
  {
    "objectID": "posts/2022-11-15-shiny-bslib-demo/2022-11-15-shiny-bslib-demo.html#git-commit-push",
    "href": "posts/2022-11-15-shiny-bslib-demo/2022-11-15-shiny-bslib-demo.html#git-commit-push",
    "title": "Build a Shiny App Demo",
    "section": "6: Git Commit & Push",
    "text": "6: Git Commit & Push\nNow that you’ve successfully published your application from the RStudio IDE, wrap up your changes and commit / push them back to your GitHub repo. There are several ways to do this (GitHub Desktop app, Terminal, Windows Powershell, etc.) but I’ve found the easiest to be within the RStudio IDE. First, you’ll want to commit (or save) a local snapshot of the edits you’ve made from the remote GitHub repo. Since this commit is saved locally, you’ll also need to push this commit back to your GitHub repo.\nUnder the Git pane, click on Commit to open a window that allows you to inspect the files that were modified, added, deleted, etc. The quickest way to stage all files for commit is to click on any of the filenames you edited in the top-left pane of the commit popup window, select all files using the Select All keystroke (Ctrl+A on Windows or Cmd+A on Mac), then click on the Stage button. Write a message in the blank top-right message box for this commit - Keep it succinct about the file(s) changes that you’ve made. Click on the Commit button under the message box and this saves a local snapshot of the changes you’ve made.\nTo push the changes back to GitHub, click on the green Push button in the top-right of the commit window and this will push your locally committed changes to your remote GitHub repo. Now, in addition to the live app that you’ve deployed, you’ll also be able to direct end users to your GitHub repo with your app’s source code."
  },
  {
    "objectID": "posts/2020-01-20-Puerto-Rico-Earthquakes/2020-01-20-Puerto-Rico-Earthquakes.html",
    "href": "posts/2020-01-20-Puerto-Rico-Earthquakes/2020-01-20-Puerto-Rico-Earthquakes.html",
    "title": "Analyzing Earthquakes in Puerto Rico",
    "section": "",
    "text": "My mother’s hometown, Ponce, has been hit badly by the recent earthquakes. A cousin asked me to “play with some data” and turns out that the USGS maintains great data on seismic activity in the Caribbean. I focused on coordinates close to PR to produce the attached visual. I’ll revisit this visualization in a few months when more data becomes available!\n\nLinks:\n\nU.S. Geologival Survey: https://www.usgs.gov/\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-07-03-Shiny-Voice-Input/2020-07-03-Shiny-Voice-Input.html",
    "href": "posts/2020-07-03-Shiny-Voice-Input/2020-07-03-Shiny-Voice-Input.html",
    "title": "Shiny Voice-Activated input",
    "section": "",
    "text": "Hey R Shiny users, want to make your Shiny apps voice-interactive? For example, “Click on Product Toy-2A and then drill-down by Region.” Ummm, yes please. Pretty neat stuff made possible with the heyshiny package. “The heyshiny package provides a new Shiny input, the speechInput(). This new input allows your Shiny app to listen to the microphone, recognize the speech, and return it as text.”\nThe main caveat with this package is that it is based on the annyang JavaScript library and therefore requires that 1) you’re online and 2) using a browser that supports speech recognition (I tried this on Chrome and works fine). It’s not on CRAN but check out the GitHub page, below. Here’s an example if you’re getting set up, pulled from their GitHub repo:\n\nSource: * heyshiny on GitHub\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2021-01-19-rsthemes/2021-01-19-rsthemes.html",
    "href": "posts/2021-01-19-rsthemes/2021-01-19-rsthemes.html",
    "title": "rsthemes: Customizing your RStudio IDE",
    "section": "",
    "text": "Feel like freshening up your RStudio IDE? Check out the rsthemes package by Garrick Aden-Buie to explore and apply different themes. For the last year or so, I’ve been enjoying the “base16 Ashes” theme at work and home. I prefer the “One Dark” theme on Atom / Juno for my Julia setup, But I’m starting to feel like I should use “One Dark” across the board. 🤓\nThis package is not yet on CRAN, so you’ll have to install via devtools::install_github(). See installation and usage code below, pulled from the rsthemes GitHub:\n\n## INSTALLATION\n# Instructions assume devtools is installed)\ndevtools::install_github(\"gadenbuie/rsthemes\")\n\n# Install custom themes + additional set of base16-based themes\nrsthemes::install_rsthemes(include_base16 = TRUE)\n\n## USAGE\n# list installed themes\nrsthemes::list_rsthemes()\n\n# Try all themes\nrsthemes::try_rsthemes()\n\n# Try just the light, dark, or base16 themes, e.g.\nrsthemes::try_rsthemes(\"light\")\n\n# Use a theme\nrstudioapi::applyTheme(\"One Dark {rsthemes}\")\n\nSource:\n\nrsthemes on GitHub\nbase16 themes\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-12-17-Hands-On-Machine-Learning-with-R/2019-12-17-Hands-On-Machine-Learning-with-R.html",
    "href": "posts/2019-12-17-Hands-On-Machine-Learning-with-R/2019-12-17-Hands-On-Machine-Learning-with-R.html",
    "title": "Book Rec: Hands-On Machine Learning with R",
    "section": "",
    "text": "I am super excited to finally dig into Hands-On Machine Learning with R by Brad Boehmke, Ph.D. and Brandon Greenwell.\nThere are a ton of solid ML books on the market for Python and R users but I’ve struggled with developing a best practice ML workflow to make maintainable code and to enhance my sampling, evaluation, and iteration process. As an R user, I’m glad to have this hands-on approach resource to improve my use of the ML stack within R.\nThe full text and code samples are available online, for free: https://lnkd.in/gvmXY3W\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-02-08-R-write-table/2020-02-08-R-write-table.html",
    "href": "posts/2020-02-08-R-write-table/2020-02-08-R-write-table.html",
    "title": "Copy R Objects to Clipboard",
    "section": "",
    "text": "You learn something new every day, even basics! For R users on Windows (I’m assuming the same holds true for Mac and Linux users), do you ever find yourself needing to quickly export a dataframe or tibble to Excel or a text editor? I recently discovered base R’s write.table function and have used it much more often than I anticipated.\nSee below syntax as an example:\n\n\n\nbase R’s write.table\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-02-01-k-fold-cross-validation/2020-02-01-k-fold-cross-validation.html",
    "href": "posts/2020-02-01-k-fold-cross-validation/2020-02-01-k-fold-cross-validation.html",
    "title": "Resampling with k-fold Cross Validation",
    "section": "",
    "text": "k-fold “randomly divides the training data into k groups (or folds) of approximate size [with the model being] fit on k-1 folds and the remaining fold used to compute model performance.” I use k=5 or k=10 given that this is typical industry practice without really questioning “why?”\n\nIn reading through Hands-On Machine Learning with R by Brad Boehmke, Ph.D. and Brandon Greenwell, I was surprised to learn that studies have shown that k=10 performs similarly to leave-one-out cross validation where k=n.\nWithout realizing it, sometimes I get carried away optimizing code and drifting from statistics and the core “science” in data science… k-fold CV is a technique used by many and is agnostic to your statistical programming language of choice but if you’re an #R user, I can’t recommend this book enough (free in full online, link below)!\nSource:\n\nHands-On Machine Learning with R Chapter 2.4 Resampling Methods\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-12-15-Java-Joy/2019-12-15-Java-Joy.html",
    "href": "posts/2019-12-15-Java-Joy/2019-12-15-Java-Joy.html",
    "title": "Spreading Joy with Java Joy",
    "section": "",
    "text": "One of my childhood friends from Atlanta, Chad Lindsey, has helped this amazing company come to SF and they’d be grateful for your business. Check Java Joy’s website or connect with them on LinkedIn (links below) to learn more.\nJava Joy’s mission is to empower people of all abilities to transform others by spreading their unmatched joy. Their vision is to become the largest employer and best place to work in the U.S. for adults with disabilities. Their vehicle is a mobile coffee cart and a passion for joy.\nInteresting fact that I learned from their website: 75% of adults with disabilities CAN and WANT to work, but only 35% are engaged in meaningful employment.\nLinks:\n\nFind more info about Java Joy here: https://java-joy.org/\nJava Joy on Linked\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "javier",
    "section": "",
    "text": "I’m a data scientist with experiences in financial modeling, analytics, and machine learning. For a living, I get to solve problems with data… Life is good!"
  },
  {
    "objectID": "index.html#programming",
    "href": "index.html#programming",
    "title": "javier",
    "section": "Programming",
    "text": "Programming\nI use R to develop and deploy packages, web apps, automation pipelines, machine learning workflows, and websites (this site was built with Quarto). My daily toolkit also includes Python but I prefer R."
  },
  {
    "objectID": "index.html#web-app-development",
    "href": "index.html#web-app-development",
    "title": "javier",
    "section": "Web App Development",
    "text": "Web App Development\nMy framework of choice for web app development is R + Shiny . With Shiny, I can build enterprise-grade UIs on top of Bootstrap 5 that can be infinitely styled with HTML, Sass, CSS, JavaScript, and more."
  },
  {
    "objectID": "index.html#data-science-communities",
    "href": "index.html#data-science-communities",
    "title": "javier",
    "section": "Data Science Communities",
    "text": "Data Science Communities\nThere are a plethora of online and in-person data science communities to learn from and share your own experiences. Online communities for open-source languages and data science are incredibly welcoming and below are my favorites:\n\nSoCal R Users Group\nData Science Hangout\nR-Ladies Global\nThe Ravit Show"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "In the past, I lost track of reality trying to track a gazillion links covering every data-science-friendly programming language under the sun. **shakes head** Bad idea. Since I program in R daily, I like to keep track of R and Posit / RStudio developments. I’m mostly going to share R resources that I find useful for analytics, statistical programming, machine learning, data science workflows, and web app development. I’m enjoying Python a lot more recently so I’ll slowly build up this resources page with Python sub-topics that I find bookmark worthy.\nIn terms of the best place to start for getting into data analysis, I recommend learning SQL as this is by far the most widely used data querying language across the corporate and academic landscapes and if you master SQL, you’ve mastered most of the transformations that are possible for tabular numeric data sets. Nonetheless, I will not cover SQL resources here as I rarely write raw SQL anymore. Instead, I use R to establish data warehouse connections and I query that raw data using the common tidyverse collection of R packages to execute SQL code in the back-end (via the dbplyr package).\nR and Python are open-source programming languages for statistical computing and graphics. These two languages have friendly online (and in-person) communities devoted to making data science easier to consume, easier to apply, and more effective at solving business problems. One of the things that I like most about both languages is the thousands of packages available making almost everything in R or Python a little easier from ETL, to method chaining, to developing predictive models and interactive web apps. I certainly welcome any suggestions that you might have for the lists below!"
  },
  {
    "objectID": "resources.html#r-books-classics",
    "href": "resources.html#r-books-classics",
    "title": "Resources",
    "section": "R Books: Classics",
    "text": "R Books: Classics\n\nR for Data Science: Phenomenal introduction to R, the RStudio IDE, and the tidyverse collection of packages\nAdvanced R: Covers concepts, methods, and advanced object-oriented structures for R\nMastering Shiny: Designed to teach the foundations of Shiny for web development and more advanced concepts such as the introduction of modules to the Shiny framework\nR Packages: The definitive reference point for R package development “covering workflow and process, alongside the presentation of all the important moving parts that make up an R package”"
  },
  {
    "objectID": "resources.html#r-books-applied-resources",
    "href": "resources.html#r-books-applied-resources",
    "title": "Resources",
    "section": "R Books: Applied Resources",
    "text": "R Books: Applied Resources\n\nTidy Modeling with R: Over the last few months, I’ve learned a lot from this A to Z resource on predictive modeling workflows using the tidymodels framework\nDeep Learning with R, Second Edition: In-depth introduction to artificial intelligence and deep learning applications with R using the Keras library\nForecasting Principles and Practice, Third Edition: Said best by the author, “The book is written for three audiences: (1) people finding themselves doing forecasting in business when they may not have had any formal training in the area; (2) undergraduate students studying business; (3) MBA students doing a forecasting elective.”\nRegression and Other Studies: Super applied textbook on advanced regression techniques, Bayesian inference, and causal inference\nSupervised Machine Learning for Text Analysis in R: Written by two Posit software engineers and incredible additions to their tidymodels team, Emil Hvitfeldt and Julia Silge, this book is a masterclass in natural language processing taking you from the basics of NLP to real-life applications including inference and prediction"
  },
  {
    "objectID": "resources.html#r-packages",
    "href": "resources.html#r-packages",
    "title": "Resources",
    "section": "R Packages",
    "text": "R Packages\n\ntidyverse: A collection of packages for data manipulation and functional programming (I use dplyr, stringr, and purrr on a daily basis)\ntidymodels: Hands-down my preferred collection of packages for building reproducible machine learning recipes, workflows, model tuning, model stacking, and cross-validation\ntidyverts: A collection of packages for time series analysis that comes out of Rob Hyndman’s lab\nDT: This is an R implementation of the popular DataTables JavaScript library that lets you build polished, configurable tables for use in web reports, slides, and Shiny apps\nbs4Dash: This R Shiny framework brings Bootstrap + AdminLTE dependencies to Shiny (including 1:1 support for shinydashboard functions) and it’s my go-to for developing enterprise-grade Shiny apps\nleaflet: R implementation of the popular Leaflet JavaScript library for developing interactive maps\nplotly: An extensive graphic library for creating interactive visualizations and 3D (WebGL) charts"
  },
  {
    "objectID": "resources.html#python-books",
    "href": "resources.html#python-books",
    "title": "Resources",
    "section": "Python Books",
    "text": "Python Books\n\nThe Quick Python Book, Third Edition: This book by Naomi Ceder is a few years old now (2018) but it’s the best end-to-end intro on Python that I’ve yet read taking you from basic classes / structures to function writing to working with modules\nPython Data Science Handbook: Introduction to the core libraries essential for working with data in Python\nEffective Pandas: Patterns for Data Manipulation: Easy to follow tutorials, at your own pace, for mastering the popular Pandas library"
  },
  {
    "objectID": "resources.html#python-packages",
    "href": "resources.html#python-packages",
    "title": "Resources",
    "section": "Python Packages",
    "text": "Python Packages\n\nNumPy: Brings the computational power of C and Fortran to Python programmers for applying high-level mathematical functions to arrays and more\nPandas: This is the most popular package for data manipulation and analysis with extended operations available for tabular and time series data\nMatplotlib: A comprehensive library for creating static, animated, and interactive visualizations in Python\nscikit-learn: Built on top of NumPy, SciPy, and matplotlib, “sklearn” makes the development of predictive analysis workflows a simple and reproducible process\nBeautiful Soup: The beautifulsoup4 library makes web scraping HTML and XML data a breeze\nStreamlit: Using pure Python, this package lets you build interactive web apps in minutes with no UI / front-end experience required"
  }
]