[
  {
    "objectID": "posts/2020-02-20-Themis/2020-02-20-Themis.html",
    "href": "posts/2020-02-20-Themis/2020-02-20-Themis.html",
    "title": "themis: Extra Steps for tidymodels + recipes",
    "section": "",
    "text": "I‚Äôve been transitioning a lot of my workflows to the tidymodels framework and I am super excited about the future of tidymodels (recipes + parsnip + dials + tune + workflow + more üò≠‚úäüôå). If you‚Äôre using recipes often like me, a new library called {themis}, by Emil Hvitfeldt expands the {recipes} pre-processing steps for working with unbalanced data sets (it adds functionality for under- and hybrid-sampling techniques). I love me some smote, and now I can incorporate this sampling technique into my recipes with themis::step_smote()!\n# Installation\ninstall.packages(\"themis\")\n\n# Example workflow\nlibrary(recipes)\nlibrary(modeldata)\nlibrary(themis)\n\ndata(okc)\n\nsort(table(okc$Class, useNA = \"always\"))\n#> \n#>  <NA>  stem other \n#>     0  9539 50316\n\nds_rec <- recipe(Class ~ age + height, data = okc) %>%\n  step_meanimpute(all_predictors()) %>%\n  step_smote(Class) %>%\n  prep()\n\nsort(table(bake(ds_rec, new_data = NULL)$Class, useNA = \"always\"))\n#> \n#>  <NA>  stem other \n#>     0 50316 50316\nSource: * themis * themis on GitHub\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-01-04-CCPA/2020-01-04-CCPA.html",
    "href": "posts/2020-01-04-CCPA/2020-01-04-CCPA.html",
    "title": "California Consumer Privacy Act",
    "section": "",
    "text": "The California Consumer Privacy Act (‚ÄúCCPA‚Äù) data rights include, but are not limited to, the following:\n\nthe right to know,\nthe right to delete,\nthe right to opt-out of personal data sale, and\nthe right to non-discrimination when a consumer exercises a privacy right under CCPA.\n\nCCPA will only apply to businesses that meet one or more of these three criteria:\n\nhave annual revenues greater than $25 million USD,\nbusinesses that buy / receive / sell personal information of 50k+ consumers, or\nbusinesses that derive 50% or more of annual revenues from selling consumers‚Äô personal information\n\nCCPA exempt businesses include HIPAA-compliant health insurers and providers, certain financial institutions, and credit reporting agencies.\nThe expected fine per unintentional and intentional violation is $2,500 and $7,500, respectively. Fines make sense, but I hope that the California Department of Justice can establish clear reporting and audit requirements to enforce these new regulations.\nSource:\n\nCNET‚Äôs oveview of CCPA\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-01-22-Systematically-Improving-Coffee/2020-01-22-Systematically-Improving-Coffee.html",
    "href": "posts/2020-01-22-Systematically-Improving-Coffee/2020-01-22-Systematically-Improving-Coffee.html",
    "title": "An Algorithm for Better Espresso",
    "section": "",
    "text": "Source:\n\nSystematically Improving Espresso: Insights from Mathematical Modeling and Experiment\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-12-31-Ubuntu-Update/2019-12-31-Ubuntu-Update.html",
    "href": "posts/2019-12-31-Ubuntu-Update/2019-12-31-Ubuntu-Update.html",
    "title": "Ubuntu Update to 19.10",
    "section": "",
    "text": "Update Ubuntu 19.04 to 19.10 on dual-booted systems. Not sure exactly where things broke down for me, but as of right now, #Ubuntu seems inaccessible. Not too bummed about it as everything is backed up, but if you‚Äôve recently had the same heartburn and found a solution, please send me a message!\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-12-04-Finland-Education-System/2019-12-04-Finland-Education-System.html",
    "href": "posts/2019-12-04-Finland-Education-System/2019-12-04-Finland-Education-System.html",
    "title": "Education Reform and Lessons from Finland",
    "section": "",
    "text": "Despite Finnish students spending the least time per week learning (and starting school later, and having much less homework), their students have some of the highest reading test scores.\nNot captured in this study, but I‚Äôd be curious to see how external factors impact these results. We have to assume American parents spend a lot less time helping their kids learn (imagine having parents that work 2 or 3 jobs to make ends meet). Or how does average classroom size differ from country to country? Or distance traveled from home to school? Or distance to local libraries? Either way, the Finnish are doing something right here.\nLinks:\n\nFinland has the most efficient education system in the world on Quartz\nGoogle‚Äôs Avinash Kaushik‚Äôs post on LinkedIn\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2021-10-21-torch-for-r/2021-10-21-torch-for-r.html",
    "href": "posts/2021-10-21-torch-for-r/2021-10-21-torch-for-r.html",
    "title": "Torch for R + luz",
    "section": "",
    "text": "Torch for R‚Ä¶ wow! üî•üî•üî• I was recently having a discussion with a coworker about the benefits of Torch, especially the power of training one global model capable of hierarchical projections (awesome for time series) and predicting multiple group-specific regressions. I went down a Googling rabbit hole last weekend and came across some amazing articles by Sigrid Keydana (see links below) introducing torch to the R community and also recently releasing luz, a high-level R interface to Torch (‚Äúluz is to torch what Keras is to TensorFlow‚Äù).\nRStudio‚Äôs MLVerse team is doing really exciting things for the R machine learning and AI community. With torch, I no longer need to launch a conda environment for complex NNs (although having Python on your system is always handy üòÖ). And even better, ‚Äútorch for R is built directly on top of libtorch, a C++ library that provides the tensor-computation and automatic-differentiation capabilities essential to building neural networks.‚Äù If you‚Äôre looking for fast NNs and deep learning solutions within the #rstats framework, give these packages a try. Happy Friday and happy learning! ü§ìüìö"
  },
  {
    "objectID": "posts/2021-10-21-torch-for-r/2021-10-21-torch-for-r.html#sources",
    "href": "posts/2021-10-21-torch-for-r/2021-10-21-torch-for-r.html#sources",
    "title": "Torch for R + luz",
    "section": "Sources",
    "text": "Sources\n\nTorch for R: https://torch.mlverse.org/\nIntro to Torch: Torch for R\nIntro to Luz: Que haja luz: More light for torch!\nIntro to TabNet: TabNet for Tabular Data with Torch\nIntro to Time Series with Torch: Introductory Time-Series Forecasting with Torch\nSigrid Keydana: Sigrid on LinkedIn"
  },
  {
    "objectID": "posts/2019-10-19-Information-is-Beautiful-Awards/2019-10-19-Information-is-Beautiful-Awards.html",
    "href": "posts/2019-10-19-Information-is-Beautiful-Awards/2019-10-19-Information-is-Beautiful-Awards.html",
    "title": "Information is Beautiful 2019 Awards",
    "section": "",
    "text": "A few weeks ago, Will Chase was a guest on Scatter Podcast to talk about his work as a researcher and data visualization professional.\nSuper exciting news for Will‚Ä¶ His visualizing earthquake risk project was shortlisted for the Information is Beautiful 2019 Awards (the Science & Technology category). Give it a look and if you like what you see, please vote for his amazing project!! Congrats Will!\nLinks:\n\nVote for Will‚Äôs project on Kantar‚Äôs Information is Beautiful 2019 Awards\nWill‚Äôs (new!) website: www.williamrchase.com\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-04-05-DnD-Adventures/2020-04-05-DnD-Adventures.html",
    "href": "posts/2020-04-05-DnD-Adventures/2020-04-05-DnD-Adventures.html",
    "title": "D&D Adventures + blogdown",
    "section": "",
    "text": "If you know of good resources for newbies and new D&D Dungeon Masters (‚ÄúDMs‚Äù), I‚Äôd love to know about your favorite resources. For data science folks out there, this site was built with R using blogdown, Hugo themes, and deployed for free using GitHub and Netlify.\n\n\n\nD&D dragon looking for trouble\n\n\nReference:  * Javier‚Äôs D&D Adventures\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-11-11-R-Markown/2019-11-11-R-Markown.html",
    "href": "posts/2019-11-11-R-Markown/2019-11-11-R-Markown.html",
    "title": "15 Tips for Making Better Use of R",
    "section": "",
    "text": "It‚Äôs such a great way to make reports come to life. I‚Äôve been using it more in the last month than I have in the last year and can‚Äôt believe how much more interactive I‚Äôve made my deliverables (and even my own data exploration). Really excited to see what new tricks I can learn from RStudio‚Äôs webinar on Friday, November 15!\nLinks:\n\nRStudio Webinar Signup on Eventbrite\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-11-04-dbplyr-2.0.0/2020-11-04-dbplyr-2.0.0.html",
    "href": "posts/2020-11-04-dbplyr-2.0.0/2020-11-04-dbplyr-2.0.0.html",
    "title": "What‚Äôs New in dbplyr 2.0.0",
    "section": "",
    "text": "Source: * tidyverse blog‚Äôs dbplyr 2.0.0 Official Announcement\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-10-22-BBC-Visualization-Journey-with-R/2019-10-22-BBC-Visualization-Journey-with-R.html",
    "href": "posts/2019-10-22-BBC-Visualization-Journey-with-R/2019-10-22-BBC-Visualization-Journey-with-R.html",
    "title": "Visual & Data Journalism at BBC",
    "section": "",
    "text": "Great read, AND super neat to see the BBC team open-source their BBPLOT and R Cookbook to help streamline your visualization workflows, reduce manual repetition of code setup for a ggplot2 viz, and to educate you on better storytelling approaches with R.\nLinks:\n\n‚ÄúHow the BBC Visual and Data Journalism team works with graphics in R‚Äù by BBC Visual and Data Journalism\nBBC‚Äôs bbplot package on GitHub\nBBC‚Äôs R Cookbook\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-04-19-Police-Killings/2020-04-19-Police-Killings.html",
    "href": "posts/2020-04-19-Police-Killings/2020-04-19-Police-Killings.html",
    "title": "Lying with Statistics",
    "section": "",
    "text": "Plot 1: Police killings by date, by race General observation: Police kill more white people than black people \nPlot 2: Police killing boxplot showing murder rates, by race, by police department General explanation and takeaway: The dots on each boxplot show the statistical outliers, box plot lines extend out to the ‚Äúmin‚Äù and ‚Äúmax‚Äù, and the box lines (from bottom to top of each box) represent the first quartile (25th percentile), median (50th percentile), and third quartile (75th percentile) \nPlot 3: Police killing boxplot, now log-transforming the murder rates to more easily identify statistical differences, by race General explanation and takeaway: Log-transforming data points for visualization or modeling purposes is a technique by which you can smooth observed data making it more robust (or resistant) to outliers. I effectively re-wrote the murder rates to show exponential relativity. Important caveat: Are Native Americans more likely to die by police than other races? Sure looks like it‚Ä¶ but see Plot 4 for more thoughts \nPlot 4: Police killing boxplot, now log-transforming the murder rates using a log base 10 (easier interpretability) and ‚Äúfixing‚Äù the Native American data points causing a misleading assumption in Plot 3, i.e., Native American death rates appeared much higher than others in Plot 3 given the fact that log(0) = 1. General takeaway: There were such few Native American data points that log-transforming all of the zeroes was unintentionally bastardizing the analysis. It would appear black people are almost an order of magnitude more likely to be killed by police than white people. \nI do not seek to answer questions of ‚Äúwhy‚Äù systemic injustice exists in the US, but I wanted to analyze police killing data and share these dialectical investigations.\nSource: * Samuel Sinyangwe & the Mapping Police Violence team\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-11-05-R-Shiny-Expert-Course/2019-11-05-R-Shiny-Expert-Course.html",
    "href": "posts/2019-11-05-R-Shiny-Expert-Course/2019-11-05-R-Shiny-Expert-Course.html",
    "title": "Shiny Developer Expert Course",
    "section": "",
    "text": "I am super excited about the new R Shiny Developer course that Matt Dancho launched, especially since he is the next guest on Scatter Podcast.\nHe‚Äôs one of the best data science educators that I‚Äôve learned from and the deeper that I dive into my data science career, the more and more value I find from his Business Science courses, Learning Labs, his online tutorials (on the tidyverse, predictive analytics, data science best practices), his R packages‚Ä¶ The guy is a beast!\nIf you are familiar with Shiny but want to gain some expert level Shiny skills, this class is for you. Great job with everything that you‚Äôre doing Matt and I‚Äôm excited to release your episode in a few days!\nLinks:\n\nBusiness Science‚Äôs Expert Shiny Developer with AWS\n15% off Business Science 4-course R bundle for Scatter Podcast listeners\nScatter Podcast Episode 25 w/ Matt Dancho\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-06-20-dplyr-1.0.0/2020-06-20-dplyr-1.0.0.html",
    "href": "posts/2020-06-20-dplyr-1.0.0/2020-06-20-dplyr-1.0.0.html",
    "title": "Testing dplyr 1.0.0",
    "section": "",
    "text": "Below is an example highlighting the new across() syntax meant to be used within a mutate() function. I tried this on a few columns of the mtcars data set with dplyr‚Äôs case_when() a la SQL CASE_WHEN (as opposed to nesting multiple if_else() statements to the point of confusion!). The dplyr changes are subtle but will definitely streamline my data wrangling.\n\n\n\ndplyr 1.0.0‚Äôs new across() function\n\n\nSource: * dplyr, a core tidyverse package\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-09-13-R-Package-Detailed-Tutorial/2020-09-13-R-Package-Detailed-Tutorial.html",
    "href": "posts/2020-09-13-R-Package-Detailed-Tutorial/2020-09-13-R-Package-Detailed-Tutorial.html",
    "title": "Detailed R Package Development Tutorial from Method Bites",
    "section": "",
    "text": "Source: * Methods Bites: How to write your own R package and publish it on CRAN\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html",
    "href": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html",
    "title": "rstudio::global tips, tricks, and more",
    "section": "",
    "text": "I attended several sessions throughout rstudio::global and below are my favorite reminders, takeaways, and new concepts that I learned. You‚Äôll gain the most from this article if you are comfortable with predictive analytics with R and have been exposed to the tidyverse and tidymodels collections of packages. If you missed the conference, all of the presentations and slides will be made available on RStudio‚Äôs website soon. Some philosophical, some technical."
  },
  {
    "objectID": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html#irregular-grid-searches",
    "href": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html#irregular-grid-searches",
    "title": "rstudio::global tips, tricks, and more",
    "section": "Irregular Grid Searches",
    "text": "Irregular Grid Searches\nIrregular grid searches help you tune your models by cycling through randomized hyperparameters in an effort to yield the best (or at least better) performance. I‚Äôve been using the dials package to create Latin hypercubes which is explained below, highlighting the benefits of this space-filling design vs a purely randomized irregular grid search. Max Kuhn‚Äôs ‚ÄúNew in tidymodels‚Äù rstudio::global event provided me with a fresh reminder about why these methods are beneficial and how they actually optimize performance.\nThe documentation below was copied almost entirely from Chapter 13 of Max Kuhn and Julia Silge‚Äôs Tidy Modeling with R (‚ÄúTMWR‚Äù). I‚Äôve deleted certain bits and added minimal helper comments in brackets. If you find yourself statistically programming, bootstrapping / resampling, improving model performance through grid searches, maybe even ensemble stacking, I‚Äôd highly recommend that you bookmark the TMWR book.\n\nRandom Grid Search\nThere a several options for creating non-regular grids. The first is to use random sampling across the range of parameters. The grid_random() function [of the dials package] generates independent uniform random numbers across the parameter ranges. If the parameter object has an associated transformation (such as we have for penalty), the random numbers are generated on the transformed scale. For example:\n\nset.seed(10)\nmlp_param %>% \n  grid_random(size = 1000) %>% # 'size' is the number of combinations\n  summary()\n#>   hidden_units      penalty           epochs    \n#>  Min.   : 1.00   Min.   :0.0000   Min.   :  10  \n#>  1st Qu.: 3.00   1st Qu.:0.0000   1st Qu.: 259  \n#>  Median : 6.00   Median :0.0000   Median : 480  \n#>  Mean   : 5.58   Mean   :0.0432   Mean   : 496  \n#>  3rd Qu.: 8.00   3rd Qu.:0.0050   3rd Qu.: 738  \n#>  Max.   :10.00   Max.   :0.9932   Max.   :1000\n\nFor penalty, the random numbers are uniform on the log (base 10) scale but the values in the grid are in the natural units.\nThe issue with random grids is that, with small-to-medium grids, random values can result in overlapping parameter combinations. Also, the random grid needs to cover the whole parameter space but the likelihood of good coverage increases with the number of grid values. Even for a sample of 15 candidate points, this plot shows some overlap between points for our example multilayer perceptron:\n\nlibrary(ggforce)\nset.seed(200)\nmlp_param %>% \n  # The 'original = FALSE' option keeps penalty in log10 units\n  grid_random(size = 15, original = FALSE) %>% \n  ggplot(aes(x = .panel_x, y = .panel_y)) + \n  geom_point() +\n  geom_blank() +\n  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + \n  labs(title = \"Random design with 15 candidates\")\n\n\n\n\nRandom grid search using dials::grid_random()\n\n\n\n\nSpace-Filling Designs\nA much better approach is to use a set of experimental designs called space-filling designs. While different design methods have slightly different goals, they generally find a configuration of points that cover the parameter space with the smallest chance of overlapping or redundant values. See Santner et al.¬†(2003) for an overview of space-filling designs.\nThe dials package contains functions for Latin hypercube and maximum entropy designs. As with dials::grid_random(), the primary inputs are the number of parameter combinations and a parameter object. Let‚Äôs compare the above random design with a Latin hypercube design for 15 candidate parameter values.\n\nset.seed(200)\nmlp_param %>% \n  grid_latin_hypercube(size = 15, original = FALSE) %>% \n  ggplot(aes(x = .panel_x, y = .panel_y)) + \n  geom_point() +\n  geom_blank() +\n  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + \n  labs(title = \"Latin Hypercube design with 15 candidates\")\n\n\n\n\nLatin hypercube designed using the dials::grid_latin_hypercube() helper\n\n\nWhile not perfect, this design spaces the points further away from one another.\nSpace-filling designs can be very effective at representing the parameter space. The default design used by the tune package is the maximum entropy design. These tend to produce grids that cover the candidate space well and drastically increase the chances of finding good results.\nTo learn more about advanced iterative search methods such as Bayesian optimization and simulated annealing, please visit Chapter 14 of TMWR. Max Kuhn covered these advanced techniques during his tidymodels rstudio::global event and I haven‚Äôt had a chance to try these iterative search methods."
  },
  {
    "objectID": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html#usemodels",
    "href": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html#usemodels",
    "title": "rstudio::global tips, tricks, and more",
    "section": "usemodels",
    "text": "usemodels\nIn the spirit of sharing useful tips, tricks, and packages, I‚Äôd like to introduce you to Max Kuhn‚Äôs usemodels package. If you‚Äôre on the fence about adopting the tidymodels framework, this package is definitely for you. Even for experienced users, usemodels is an excellent way to quickly generate boilerplate code snippets that are algorithm-specific. This package is not yet on CRAN, so install it using the following:\n\n# Installing usemodels from its GitHub repo\ndevtools::install_github(\"tidymodels/usemodels\")\n\nGiven a simple formula and a data set, the use_* functions can create code that is appropriate for the data (given the model). [Note: The model formula will be in the form of y ~ a + b + c or y ~ . if you plan on including all available variables in your model.]\nFor example, using the palmerpenguins data with a glmnet model:\n\nlibrary(usemodels)\nlibrary(palmerpenguins)\ndata(penguins)\nuse_glmnet(body_mass_g ~ ., data = penguins)\n\n# NOTE: The below will be printed in your console with your model recipe and tailored\n# with the required pre-processing steps given your algorithm of choice (in this case,\n# glmnet). The `usemodels` output also provides the code structure for a reproducible \n# workflow, made possible with the `workflows` package. Should you choose to tune\n# your model, `usemodels` also provides code snippets for producing a grid of parameter\n# combinations to use with your hyperparameter tuning grid search.\n\nglmnet_recipe <- \n  recipe(formula = body_mass_g ~ ., data = penguins) %>% \n  step_novel(all_nominal(), -all_outcomes()) %>% \n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors(), -all_nominal()) \n\nglmnet_spec <- \n  linear_reg(penalty = tune(), mixture = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\") \n\nglmnet_workflow <- \n  workflow() %>% \n  add_recipe(glmnet_recipe) %>% \n  add_model(glmnet_spec) \n\nglmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), mixture = c(0.05, \n    0.2, 0.4, 0.6, 0.8, 1)) \n\nglmnet_tune <- \n  tune_grid(glmnet_workflow, resamples = stop(\"add your rsample object\"), grid = glmnet_grid)\n\nAs of today‚Äôs date, this package includes templates available with the following code: use_cubist, use_earth, use_glmnet, use_kknn, use_ranger, and use_xgboost"
  },
  {
    "objectID": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html#stacks",
    "href": "posts/2021-01-24-rstudio-global/2021-01-24-rstudio-global.html#stacks",
    "title": "rstudio::global tips, tricks, and more",
    "section": "stacks",
    "text": "stacks\n\nEnsemble Models\nSimilar to usemodels, this is a very early-stage package that you should start following. There are already several AI tools for producing ‚Äúmodel stacks‚Äù or ‚Äúensemble models‚Äù such as H2O.ai, DataRobot, and a few others, however, stacks is the first purpose-built for use with tidymodels. The following narrative is copied almost directly from the stacks website.\nstacks is an R package for model stacking that aligns with the tidymodels. Model stacking is an ensembling method that takes the outputs of many models and combines them to generate a new model - referred to as an ensemble in this package - that generates predictions informed by each of its members.\nThe process goes something like this:\n\nDefine candidate ensemble members using functionality from rsample, parsnip, workflows, recipes, and tune\nInitialize a data_stack object with stacks()\nIteratively add candidate ensemble members to the data_stack with add_candidates()\nEvaluate how to combine their predictions with blend_predictions()\nFit candidate ensemble members with non-zero stacking coefficients with fit_members()\nPredict on new data with predict()\n\n\n\nstacks Grammar\nAt the highest level, ensembles are formed from model definitions. In this package, model definitions are an instance of a minimal workflow, containing a model specification (as defined in the parsnip package) and, optionally, a preprocessor (as defined in the recipes package). Model definitions specify the form of candidate ensemble members.\n\nTo be used in the same ensemble, each of these model definitions must share the same resample. This rsample rset object, when paired with the model definitions, can be used to generate the tuning/fitting results objects for the candidate ensemble members with tune.\n\nCandidate members first come together in a data_stack object through the add_candidates() function. Principally, these objects are just tibbles, where the first column gives the true outcome in the assessment set (the portion of the training set used for model validation), and the remaining columns give the predictions from each candidate ensemble member. (When the outcome is numeric, there‚Äôs only one column per candidate ensemble member. Classification requires as many columns per candidate as there are levels in the outcome variable.) They also bring along a few extra attributes to keep track of model definitions.\n\nThen, the data stack can be evaluated using blend_predictions() to determine to how best to combine the outputs from each of the candidate members. In the stacking literature, this process is commonly called meta-learning.\nThe outputs of each member are likely highly correlated. Thus, depending on the degree of regularization you choose, the coefficients for the inputs of (possibly) many of the members will zero out‚Äîtheir predictions will have no influence on the final output, and those terms will thus be thrown out.\n\nThese stacking coefficients determine which candidate ensemble members will become ensemble members. Candidates with non-zero stacking coefficients are then fitted on the whole training set, altogether making up a model_stack object.\n\nThis model stack object, outputted from fit_members(), is ready to predict on new data! The trained ensemble members are often referred to as base models in the stacking literature. To learn more about how to use stacks, check out the following excellent vignettes from the tidymodels team:\n\nGetting Started with stacks\nClassification Models with stacks\n\nSources:\n\nrstudio::global(2021)\nTidy Modeling with R\ntidymodels\nusemodels\nstacks"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html",
    "title": "Beautiful Maps with Rayshader",
    "section": "",
    "text": "R, rayshader, OpenStreetMap‚Ä¶ Lots of amazing learning to be had with these mapping tools. I‚Äôm following Tyler Morgan-Wall PhD‚Äôs Adding Open Street Map Data to Rayshader Maps in R tutorial and learning a lot about the development of beautiful maps with R. Laguna Beach is one of my favorite cities in the USA (and the world!) so I applied the concepts from this tutorial to building 2D and 3D maps of Laguna Beach. My work-in-progress and screen captures are below.\nTyler Morgan-Wall PhD (TMW) authored the rayshader package and his step-by-step tutorial is both easy to follow and incredibly informative. If you get stuck (more like, ‚Äúwhen you get stuck‚Äù), you can almost always find an answer on the rayshader website or GitHub repo (excellent technical documentation and code examples).\nOne last caveat before I dive into my code - Finding high-quality LIDAR or Digital Elevation Models (DEMs) for United States cities is not easy. To apply the steps below to your location of choice, you‚Äôll have to get creative about cropping LIDAR/DEMs to zoom into the geography that you want to model (more info on cropping in the sections below)."
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#libraries",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#libraries",
    "title": "Beautiful Maps with Rayshader",
    "section": "Libraries",
    "text": "Libraries\n\n# Install dev packages\ndevtools::install_github(\"tylermorganwall/rayshader\")\ndevtools::install_github(\"tylermorganwall/rayimage\")\n\n# Load libraries\nlibrary(rayshader)\nlibrary(raster)\nlibrary(osmdata)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Source elevation data\nlaguna <- raster::raster(\"./USGS_one_meter_x42y372_CA_SoCal_Wildfires_B1_2018.tif\")"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#eda",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#eda",
    "title": "Beautiful Maps with Rayshader",
    "section": "EDA",
    "text": "EDA\nTo better explore the data, lets begin by visualizing the elevation data with rayshader defaults to understand the topography that we are working with. Before we can render this visual, we need to transform the laguna data object (assigned above) to a matrix. For faster rendering and prototyping, you can also resize the matrix using rayshader::resize_matrix(). This resizing will downscale your full dataset using bilinear interpolation.\n\n# Transform the spatial data structure into a regular R matrix\nlaguna_mat <- rayshader::raster_to_matrix(laguna)\n\n# Create small version of matrix for quick visual prototyping\nlaguna_small <- rayshader::resize_matrix(laguna_mat, 0.1)\n\n# Plot base map using basic color defaults\nlaguna_small %>% \n  rayshader::height_shade() %>% \n  rayshader::plot_map()\n\n\n\n\nUSGS Laguna Canyon LIDAR"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#cropping",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#cropping",
    "title": "Beautiful Maps with Rayshader",
    "section": "Cropping",
    "text": "Cropping\nI wanted to focus on downtown Laguna Beach but this only represents a small fraction of the raw LIDAR. To get the desired lat-long range of downtown Laguna Beach, I used Google Maps to get the endpoints of my desired box. To accomplish this cropping, below, we relied on a custom cropping function that TMW shared on the rayshader site.\n\n# Crop to fit desired box around downtown Laguna Beach\nlat_range <- c(33.541370925562376, 33.55945552155357)\nlong_range <- c(-117.79622401045448, -117.77493476466955)\n\nconvert_coords <- function(lat, long, from = CRS(\"+init=epsg:4326\"), to) {\n  data = data.frame(long = long, lat = lat)\n  sp::coordinates(data) <- ~ long + lat\n  sp::proj4string(data) = from\n  # Convert to coordinate system specified by EPSG code\n  xy = data.frame(sp::spTransform(data, to))\n  colnames(xy) = c(\"x\",\"y\")\n  return(unlist(xy))\n}\n\nraster::crs(laguna)\n\nutm_bbox <- convert_coords(lat = lat_range,\n                           long = long_range,\n                           to = crs(laguna))\n\nraster::extent(laguna)\n\nextent_zoomed <- raster::extent(utm_bbox[1], utm_bbox[2], \n                                utm_bbox[3], utm_bbox[4])h\n\nlaguna_zoom <- raster::crop(laguna, extent_zoomed)\n\nlaguna_zoom_mat <- rayshader::raster_to_matrix(laguna_zoom)"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#base-map",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#base-map",
    "title": "Beautiful Maps with Rayshader",
    "section": "Base Map",
    "text": "Base Map\nI was inspired by the relief shading work of Eduard Imhof, Swiss cartographer and mapping visionary. In particular, I loved the blueish-pink palette that Imhof popularized. When you get to this step, try adjusting the settings below to your liking. We will continue to build on top of this initial base map layer.\n\nbase_map <- laguna_zoom_mat %>% \n  rayshader::height_shade() %>% \n  rayshader::add_overlay(\n    rayshader::sphere_shade(\n      laguna_zoom_mat, \n      texture = rayshader::create_texture(\"#f5dfca\",\"#63372c\",\"#dfa283\",\"#195f67\",\"#c2d1cf\",\n                                          cornercolors = c(\"#ffc500\", \"#387642\", \"#d27441\",\"#296176\")),\n      sunangle = 0, \n      colorintensity = 5)) %>%\n  rayshader::add_shadow(rayshader::lamb_shade(laguna_zoom_mat), 0.2) %>%\n  rayshader::add_overlay(\n    rayshader::generate_altitude_overlay(\n      rayshader::height_shade(laguna_zoom_mat, texture = \"#91aaba\"),\n      laguna_zoom_mat,\n      start_transition = min(laguna_zoom_mat)-200,\n      end_transition = max(laguna_zoom_mat)))\n\nrayshader::plot_map(base_map)\n\n\n\n\nLaguna Beach base map a la Eduard Imhof"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#openstreetmap",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#openstreetmap",
    "title": "Beautiful Maps with Rayshader",
    "section": "OpenStreetMap",
    "text": "OpenStreetMap\nOpenStreetMap (OSM) offers an open-source, editable geographic database of the world and the underlying data includes details for roads, buildings, hiking paths, rivers, etc. We‚Äôll leverage the osmdata package for R to import OSM data (and utilizing the overpass API). The osmdata package processes routines in C++ for fast construction and loading into R. We‚Äôll first load several layers of OSM details. Subsequently, we‚Äôll convert those sf objects to the coordinate system we‚Äôre working with for this rayshader project.\n\nosm_bbox = c(long_range[1],lat_range[1], long_range[2],lat_range[2])\n\nlaguna_highway <- osmdata::opq(osm_bbox) %>% \n  osmdata::add_osm_feature(\"highway\") %>% \n  osmdata::osmdata_sf() \n\n# Transform coordinates to new projection\nlaguna_lines <- sf::st_transform(laguna_highway$osm_lines,\n                                 crs = raster::crs(laguna))\n\n# View streets as a ggplot2 render \nggplot(laguna_lines, aes(color = osm_id)) + \n  geom_sf() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Open Street Map `highway` attribute in Laguna Beach\")\n\n\n\n\nOSM render of Laguna Beach roads and highways using ggplot2"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#layering",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#layering",
    "title": "Beautiful Maps with Rayshader",
    "section": "Layering",
    "text": "Layering\nTo begin, lets simply layer all the ‚Äúhighway‚Äù OSM attributes on top of our base map using white lines to represent the lines.\n\n# Transform sf LINESTRING geometry and create semi-transparent overlay\nbase_map %>% \n  rayshader::add_overlay(\n    rayshader::generate_line_overlay(\n      laguna_lines, extent = extent_zoomed,\n      linewidth = 3, color = \"white\",\n      heightmap = laguna_zoom_mat)) %>% \n  rayshader::plot_map()\n\n\n\n\nBase Map Layer 1\n\n\nI want different styles for different OSM lines so let‚Äôs continue by creating several groups of OSM lines.\n\n# Subset layers of laguna_lines\nlaguna_trails <- laguna_lines %>% \n  dplyr::filter(highway %in% c(\"path\", \"bridleway\", \"steps\", \"track\"))\n\nlaguna_footpaths <- laguna_lines %>% \n  dplyr::filter(highway %in% c(\"footway\"))\n\nlaguna_roads <- laguna_lines %>% \n  dplyr::filter(highway %in% c(\"unclassified\", \"primary\", \n                               \"primary_link\", \"secondary\", \n                               \"tertiary\", \"residential\", \"service\"))\n\n# Create one encompassing layer with independent element styling\ntrails_layer <- \n  rayshader::generate_line_overlay(\n    \n    # Pink footpaths\n    laguna_footpaths, extent = extent_zoomed,\n    linewidth = 4, color = \"pink\",\n    heightmap = laguna_zoom_mat) %>%\n  \n  # Note: While barely visible in the final outputs, the following\n  # code adds an offset black shadow to the white dashed lines to\n  # represent the hiking trails around Laguna Beach \n  rayshader::add_overlay(\n    rayshader::generate_line_overlay(\n      laguna_trails, extent = extent_zoomed,\n      linewidth = 4, color = \"black\", lty = 3, offset = c(2,-2),\n      heightmap = laguna_zoom_mat)) %>%\n  rayshader::add_overlay(\n    rayshader::generate_line_overlay(\n      laguna_trails, extent = extent_zoomed,\n      linewidth = 4, color = \"white\", lty = 3,\n      heightmap = laguna_zoom_mat)) %>%\n  \n  # White roads\n  rayshader::add_overlay(\n    rayshader::generate_line_overlay(\n      laguna_roads, extent = extent_zoomed,\n      linewidth = 3, color = \"white\",\n      heightmap = laguna_zoom_mat))\n\nWe‚Äôll create some additional projections of OSM layers.\n\nlaguna_parking <- osmdata::opq(osm_bbox) %>% \n  osmdata::add_osm_feature(\"parking\") %>% \n  osmdata::osmdata_sf() \n\nlaguna_building <- osmdata::opq(osm_bbox) %>% \n  osmdata::add_osm_feature(\"building\") %>% \n  osmdata::osmdata_sf()\n\nlaguna_parking_poly <- st_transform(laguna_parking$osm_polygons, crs = crs(laguna))\nlaguna_building_poly <- st_transform(laguna_building$osm_polygons, crs = crs(laguna))"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#final-touches",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#final-touches",
    "title": "Beautiful Maps with Rayshader",
    "section": "Final Touches",
    "text": "Final Touches\nNow we‚Äôre ready to build upon our base map and see the 2D visual in action. Let‚Äôs create a polygon layer of the parking and building features, and adding the styled trails_layer on top of that.\n\npolygon_layer <- \n  rayshader::generate_polygon_overlay(laguna_parking_poly, \n                                      extent = extent_zoomed,\n                                      heightmap = laguna_zoom_mat, \n                                      palette = \"grey30\") %>%\n  rayshader::add_overlay(\n    rayshader::generate_polygon_overlay(laguna_building_poly, \n                                        extent = extent_zoomed,\n                                        heightmap = laguna_zoom_mat, \n                                        palette = \"lightgrey\"))\n\nbase_map %>% \n  rayshader::add_overlay(polygon_layer) %>%\n  rayshader::add_overlay(trails_layer) %>%\n  rayshader::plot_map()\n\n\n\n\n2D render of Laguna Beach‚Ä¶ Success!"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#features",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#features",
    "title": "Beautiful Maps with Rayshader",
    "section": "Features",
    "text": "Features\nTo render a 3D map, we follow a very similar pipeline to the 2D map but utilizing rayshader::plot_3d(). This function introduces new arguments and I spent much more time than I had anticipated in adjusting the zscale, zoom, rotation, pitch, and water specifications.\nTo exaggerate the elevation details in this area, I decided on a zscale < 1 that would still resemble the reality and beauty of the Laguna Beach hills. Too low of a zscale, e.g., zscale = 0.25, was too exaggerated for my liking. Once the 3D viewer renders your visual, enter rayshader::render_snapshot() into your console to take a flat snapshot of your 3D rendering, saving your snapshot if desired.\n\nbase_map %>% \n  rayshader::add_overlay(polygon_layer) %>%\n  rayshader::add_overlay(trails_layer) %>%\n  rayshader::plot_3d(heightmap = laguna_zoom_mat, \n                     zscale = 0.75, fov = 0, \n                     theta = -45, zoom = 0.75, phi = 45,\n                     water = TRUE, waterdepth = 4, \n                     wateralpha = 0.4, watercolor = \"lightblue\",\n                     waterlinecolor = \"white\", waterlinealpha = 0.5,\n                     background = \"white\")\n\n# Create 2D snapshot of 3D rendering\nrayshader::render_snapshot()\n\n\n\n\n3D render of Laguna Beach"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#next-steps",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#next-steps",
    "title": "Beautiful Maps with Rayshader",
    "section": "Next Steps",
    "text": "Next Steps\nI‚Äôve seen some neat examples where developers add 3D polygons to their maps to represent residential housing, commercial properties, and public facilities. I recently read about methods for rendering much higher-resolution maps (this would be great for printing). Creating 3D flyovers with rayshader also looks surprisingly high-end for open-source software. These are all things I hope to revisit at some point in the near future. TMW knocked it out of the park with his package and tutorials, so I hope you find his content as useful as I did."
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#session-info",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#session-info",
    "title": "Beautiful Maps with Rayshader",
    "section": "Session Info",
    "text": "Session Info\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.3.5    dplyr_1.0.7      sf_1.0-2         osmdata_0.1.6   \n[5] raster_3.4-13    sp_1.4-5         rayshader_0.26.1\n\nloaded via a namespace (and not attached):\n [1] rgl_0.107.14       Rcpp_1.0.7         lubridate_1.7.10   lattice_0.20-44   \n [5] png_0.1-7          prettyunits_1.1.1  class_7.3-19       ps_1.6.0          \n [9] assertthat_0.2.1   rprojroot_2.0.2    digest_0.6.27      foreach_1.5.1     \n[13] utf8_1.2.2         R6_2.5.1           e1071_1.7-8        httr_1.4.2        \n[17] pillar_1.6.2       rlang_0.4.11       progress_1.2.2     curl_4.3.2        \n[21] callr_3.7.0        magick_2.7.3       pkgdown_1.6.1      desc_1.3.0        \n[25] devtools_2.4.2     rgdal_1.5-23       htmlwidgets_1.5.3  munsell_0.5.0     \n[29] proxy_0.4-26       compiler_4.1.1     xfun_0.25          pkgconfig_2.0.3   \n[33] pkgbuild_1.2.0     htmltools_0.5.2    tidyselect_1.1.1   tibble_3.1.4      \n[37] codetools_0.2-18   fansi_0.5.0        crayon_1.4.1       withr_2.4.2       \n[41] grid_4.1.1         gtable_0.3.0       jsonlite_1.7.2     lifecycle_1.0.0   \n[45] DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       units_0.7-2       \n[49] KernSmooth_2.23-20 cli_3.0.1          cachem_1.0.6       farver_2.1.0      \n[53] fs_1.5.0           remotes_2.4.0      doParallel_1.0.16  testthat_3.0.4    \n[57] xml2_1.3.2         ellipsis_0.3.2     generics_0.1.0     vctrs_0.3.8       \n[61] iterators_1.0.13   tools_4.1.1        rayimage_0.6.2     glue_1.4.2        \n[65] purrr_0.3.4        hms_1.1.0          processx_3.5.2     pkgload_1.2.1     \n[69] parallel_4.1.1     fastmap_1.1.0      colorspace_2.0-2   sessioninfo_1.1.1 \n[73] classInt_0.4-3     rvest_1.0.1        memoise_2.0.0      knitr_1.33        \n[77] usethis_2.0.1"
  },
  {
    "objectID": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#resources",
    "href": "posts/2021-09-07-rayshader/2021-09-07-rayshader.html#resources",
    "title": "Beautiful Maps with Rayshader",
    "section": "Resources",
    "text": "Resources\n\nrayshader: https://www.rayshader.com/\nrayrender: http://www.rayrender.net/\nTyler Morgan-Wall, PhD: https://www.tylermw.com/"
  },
  {
    "objectID": "posts/2020-12-19-R-Package-One-Hour-Tutorial/2020-12-19-R-Package-One-Hour-Tutorial.html",
    "href": "posts/2020-12-19-R-Package-One-Hour-Tutorial/2020-12-19-R-Package-One-Hour-Tutorial.html",
    "title": "One-Hour R Package Development Tutorial by Shannon Pileggi, PhD",
    "section": "",
    "text": "Using functions from just the devtools and usethis packages, the whole process is much more streamlined than what I would have thought. Check out the rest of Shannon‚Äôs Piping Hot Data website as well‚Ä¶ It‚Äôs a great resource for R programmers and researchers, and she built it with the newly reintroduced distill package, just like this site. :-)\nSource: * Shannon‚Äôs Piping Hot Data ‚ÄúYour first R package in 1 hour‚Äù post * Shannon‚Äôs LinkedIn\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-01-01-Holidays/2020-01-01-Holidays.html",
    "href": "posts/2020-01-01-Holidays/2020-01-01-Holidays.html",
    "title": "Holiday Expectations vs Reality",
    "section": "",
    "text": "Holiday reality: Make a million espressos, eat loads of chocolate and cake, play more ‚ÄòThe Legend of Zelda: Link‚Äôs Awakening‚Äô than I thought was humanly possible‚Ä¶\nLife is good folks, Merry Christmas and I hope that you got to enjoy some time with family and friends this holiday season!\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-01-20-Puerto-Rico-Earthquakes/2020-01-20-Puerto-Rico-Earthquakes.html",
    "href": "posts/2020-01-20-Puerto-Rico-Earthquakes/2020-01-20-Puerto-Rico-Earthquakes.html",
    "title": "Analyzing Earthquakes in Puerto Rico",
    "section": "",
    "text": "Links:\n\nU.S. Geologival Survey: https://www.usgs.gov/\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2021-05-03-DT/2021-05-03-DT.html",
    "href": "posts/2021-05-03-DT/2021-05-03-DT.html",
    "title": "DT: An R Interface to the JavaScript library DataTables",
    "section": "",
    "text": "If you‚Äôre a Shiny dev, check out the live demos sourced below.\n\nSource:\n\nDT on GitHub\nDT site\nLive Shiny demo: DT Selections Ex. 1\nLive Shiny demo: DT Selection Ex. 2\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-07-03-Shiny-Voice-Input/2020-07-03-Shiny-Voice-Input.html",
    "href": "posts/2020-07-03-Shiny-Voice-Input/2020-07-03-Shiny-Voice-Input.html",
    "title": "Shiny Voice-Activated input",
    "section": "",
    "text": "The main caveat with this package is that it is based on the annyang JavaScript library and therefore requires that 1) you‚Äôre online and 2) using a browser that supports speech recognition (I tried this on Chrome and works fine). It‚Äôs not on CRAN but check out the GitHub page, below. Here‚Äôs an example if you‚Äôre getting set up, pulled from their GitHub repo:\n\nSource: * heyshiny on GitHub\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-11-18-Blogdown/2019-11-18-Blogdown.html",
    "href": "posts/2019-11-18-Blogdown/2019-11-18-Blogdown.html",
    "title": "Building a Website with Blogdown",
    "section": "",
    "text": "Pablo Barajas, a recent graduate at UC Irvine and friend of Scatter Podcast, led a presentation on how to build a website using R, RStudio, and Blogdown.\nFor anyone wanting to fairly easily build an online portfolio or blog that looks professional, I highly recommend Blogdown. I used this process to build this website, as did Pablo for his personal website, and I can help if you have any questions. Here‚Äôs the process in short: RStudio site build with Blogdown -> Commit to GitHub -> Apply Hugo theme -> Netlify for CI/CD\nThis event was hosted by the Orange County R Users Group (‚ÄúOCRUG‚Äù) and UCI‚Äôs Merage Analytics Club (‚ÄúMAC‚Äù). If you‚Äôre in Orange County and looking to network and learn with local data science practitioners and students, I highly recommend attending OCRUG events, R-Ladies Irvine events, or public MAC events. Great learning opportunities here!\nLinks:\n\nPablo‚Äôs presentation on his Blogdown site\nBlogdown overview from RStudio\nOCRUG‚Äôs homepage\nR-Ladies Irvine chapter\nMerage Analytics Club website\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2021-01-19-rsthemes/2021-01-19-rsthemes.html",
    "href": "posts/2021-01-19-rsthemes/2021-01-19-rsthemes.html",
    "title": "rsthemes: Customizing your RStudio IDE",
    "section": "",
    "text": "This package is not yet on CRAN, so you‚Äôll have to install via devtools::install_github(). See installation and usage code below, pulled from the rsthemes GitHub:\n\n## INSTALLATION\n# Instructions assume devtools is installed)\ndevtools::install_github(\"gadenbuie/rsthemes\")\n\n# Install custom themes + additional set of base16-based themes\nrsthemes::install_rsthemes(include_base16 = TRUE)\n\n## USAGE\n# list installed themes\nrsthemes::list_rsthemes()\n\n# Try all themes\nrsthemes::try_rsthemes()\n\n# Try just the light, dark, or base16 themes, e.g.\nrsthemes::try_rsthemes(\"light\")\n\n# Use a theme\nrstudioapi::applyTheme(\"One Dark {rsthemes}\")\n\nSource:\n\nrsthemes on GitHub\nbase16 themes\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-01-18-Create-R-Packages/2020-01-18-Create-R-Packages.html",
    "href": "posts/2020-01-18-Create-R-Packages/2020-01-18-Create-R-Packages.html",
    "title": "Create Your Own R Package",
    "section": "",
    "text": "A coworker sent me this link and I‚Äôm super excited to start developing some packages. Great timing as one of my 2020 resolutions is to write my own package(s)!\n\nLinks:\n\nR Packages by Hadley Wickham and Jenny Bryan\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-12-17-Hands-On-Machine-Learning-with-R/2019-12-17-Hands-On-Machine-Learning-with-R.html",
    "href": "posts/2019-12-17-Hands-On-Machine-Learning-with-R/2019-12-17-Hands-On-Machine-Learning-with-R.html",
    "title": "Book Rec: Hands-On Machine Learning with R",
    "section": "",
    "text": "I am super excited to finally dig into Hands-On Machine Learning with R by Brad Boehmke, Ph.D. and Brandon Greenwell.\nThere are a ton of solid ML books on the market for Python and R users but I‚Äôve struggled with developing a best practice ML workflow to make maintainable code and to enhance my sampling, evaluation, and iteration process. As an R user, I‚Äôm glad to have this hands-on approach resource to improve my use of the ML stack within R.\nThe full text and code samples are available online, for free: https://lnkd.in/gvmXY3W\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-02-04-Hadley-Wickham-on-SDS-Podcast/2020-02-04-Hadley-Wickham-on-SDS-Podcast.html",
    "href": "posts/2020-02-04-Hadley-Wickham-on-SDS-Podcast/2020-02-04-Hadley-Wickham-on-SDS-Podcast.html",
    "title": "Hadley Wickham on SuperDataScience Podcast",
    "section": "",
    "text": "This episode is a great listen as he explores RStudio Python and R integration and the future of data science.\n\nSource:\n\nSuperDataScience ep.337 with Hadley Wickham\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-02-08-R-write-table/2020-02-08-R-write-table.html",
    "href": "posts/2020-02-08-R-write-table/2020-02-08-R-write-table.html",
    "title": "Copy R Objects to Clipboard",
    "section": "",
    "text": "See below syntax as an example:\n\n\n\nbase R‚Äôs write.table\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-01-09-Writing-to-Excel-from-R/2020-01-09-Writing-to-Excel-from-R.html",
    "href": "posts/2020-01-09-Writing-to-Excel-from-R/2020-01-09-Writing-to-Excel-from-R.html",
    "title": "Writing to Excel from R",
    "section": "",
    "text": "Here is a high-level code example:\nlibrary(openxlsx)\n\n# Name the Excel worksheets where you want to export data to\nlist_of_datasets <- list(\"Overall Summary\" = df_1, \"LOB Summary\" = df_2, \"Details\" = df_3)\n\n# Save the Excel workbook to your wd with your worksheet list and name the Excel file\nwrite.xlsx(list_of_datasets, \"Excel_Workbook_2017.01.20.xlsx\")\nLinks:\n\nEzekiel‚Äôs openxlsx tutorial on RPubs\nopenxlsx repository on GitHub\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-02-01-k-fold-cross-validation/2020-02-01-k-fold-cross-validation.html",
    "href": "posts/2020-02-01-k-fold-cross-validation/2020-02-01-k-fold-cross-validation.html",
    "title": "Resampling with k-fold Cross Validation",
    "section": "",
    "text": "In reading through Hands-On Machine Learning with R by Brad Boehmke, Ph.D.¬†and Brandon Greenwell, I was surprised to learn that studies have shown that k=10 performs similarly to leave-one-out cross validation where k=n.\nWithout realizing it, sometimes I get carried away optimizing code and drifting from statistics and the core ‚Äúscience‚Äù in data science‚Ä¶ k-fold CV is a technique used by many and is agnostic to your statistical programming language of choice but if you‚Äôre an #R user, I can‚Äôt recommend this book enough (free in full online, link below)!\nSource:\n\nHands-On Machine Learning with R Chapter 2.4 Resampling Methods\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2021-05-24-dataframe/2021-05-24-dataframe.html",
    "href": "posts/2021-05-24-dataframe/2021-05-24-dataframe.html",
    "title": "A Brief History of the Dataframe",
    "section": "",
    "text": "Source:\n\nTowards Data Science: Preventing the Death of the Dataframe\npandas for Python\ntibble for R\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-12-15-Java-Joy/2019-12-15-Java-Joy.html",
    "href": "posts/2019-12-15-Java-Joy/2019-12-15-Java-Joy.html",
    "title": "Spreading Joy with Java Joy",
    "section": "",
    "text": "Java Joy‚Äôs mission is to empower people of all abilities to transform others by spreading their unmatched joy. Their vision is to become the largest employer and best place to work in the U.S. for adults with disabilities. Their vehicle is a mobile coffee cart and a passion for joy.\nInteresting fact that I learned from their website: 75% of adults with disabilities CAN and WANT to work, but only 35% are engaged in meaningful employment.\nLinks:\n\nFind more info about Java Joy here: https://java-joy.org/\nJava Joy on Linked\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-12-31-Raspberry-Pi/2020-12-31-Raspberry-Pi.html",
    "href": "posts/2020-12-31-Raspberry-Pi/2020-12-31-Raspberry-Pi.html",
    "title": "Preparing for 2021 Goals with a Raspberry Pi",
    "section": "",
    "text": "Below are some pics of my current progress.\n\n\n\nRaspberry Pi 4, Model B, 8GB RAM, & Super Cute Tiny Heat Sinks\n\n\n\n\n\nEnclosing the Pi 4 in a CanaKit case + fan\n\n\n\n\n\nRaspberry Pi OS, a Debian distro for the Pi\n\n\n\n\n\nJupyter Lab server on the Pi set up w/ Python, R, and Julia\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "javier",
    "section": "",
    "text": "I‚Äôm a data scientist with experiences in financial modeling, analytics, and machine learning. For a living, I get to solve problems with data‚Ä¶ Life is good! üßë‚ÄçüíªüöÄ"
  },
  {
    "objectID": "index.html#programming",
    "href": "index.html#programming",
    "title": "javier",
    "section": "Programming",
    "text": "Programming\nI use R to develop and deploy packages, web apps, automation pipelines, machine learning workflows, and websites (this site was built with Quarto). My daily toolkit also includes Python but I prefer R."
  },
  {
    "objectID": "index.html#web-app-development",
    "href": "index.html#web-app-development",
    "title": "javier",
    "section": "Web App Development",
    "text": "Web App Development\nMy framework of choice for web app development is R + Shiny . With Shiny, I can build enterprise-grade UIs on top of Bootstrap 5 that can be infinitely styled with HTML, Sass, CSS, JavaScript, and more."
  },
  {
    "objectID": "index.html#data-science-communities",
    "href": "index.html#data-science-communities",
    "title": "javier",
    "section": "Data Science Communities",
    "text": "Data Science Communities\nThere are a plethora of online and in-person data science communities to learn from and share your own experiences. Online communities for open-source languages and data science are incredibly welcoming and below are my favorites:\n\nSoCal R Users Group\nData Science Hangout\nR-Ladies Global\nThe Ravit Show"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Tulum, Quintana Roo, Mexico - The second happiest place on Earth, after Disneyland"
  },
  {
    "objectID": "about.html#professional",
    "href": "about.html#professional",
    "title": "About Me",
    "section": "Professional",
    "text": "Professional\nMy career includes over 15 years of data science, financial strategy, and business analytics for EY, PG&E, KPMG, Centene, and Bloomreach. I use the R and Python programming languages on a daily basis but am much more advanced with my R work. My teams apply data science techniques to empower decision makers with descriptive, predictive, and prescriptive analytics. I help bridge the gap between business needs and IT to develop and deploy automation solutions, predictive algorithms, and interactive web apps."
  },
  {
    "objectID": "about.html#scatter-podcast",
    "href": "about.html#scatter-podcast",
    "title": "About Me",
    "section": "Scatter Podcast",
    "text": "Scatter Podcast\nIn 2019, I launched Scatter Podcast to share career tips and insights from data science leaders for students, business managers, and professionals looking to pivot into data science. It was a fun and incredibly rewarding side project but after 30 episodes, I started to feel fatigued with the amount of time it took to plan, record, edit, market, etc. The podcast is on hold but not dead!"
  },
  {
    "objectID": "about.html#programming-analytics-skills",
    "href": "about.html#programming-analytics-skills",
    "title": "About Me",
    "section": "Programming & Analytics Skills",
    "text": "Programming & Analytics Skills\n\nR: tidyverse, tidymodels, Quarto, Shiny, Plotly, Leaflet, Prophet, xaringan, XGBoost, Ranger, and many more\nOther Analytics & Programming Platforms: Python, Jupyter Notebooks, h2o, GitLab / GitHub, RStudio Connect, AWS S3, AWS Redshift, Google Cloud Platform, BigQuery, SQL, Netlify\nMicrosoft: Expert-level Excel financial modeler with experiences including nested formulas, XLOOKUPs, Solver for optimization and simulations (yes‚Ä¶ in Excel!), macros for psuedo-automated data manipulation, add-in integrations, and more"
  },
  {
    "objectID": "about.html#media-appearances",
    "href": "about.html#media-appearances",
    "title": "About Me",
    "section": "Media Appearances",
    "text": "Media Appearances\n\nUC Irvine‚Äôs inaugural Latinx Initiative Conference (2021-10-15)\nData Points: Healthcare and Finance virtual conference hosted by Grid Dynamics (2021-06-09)\n\nWatch my presentation Healthcare Finance Beyond Excel: R for Automation, Time Series Forecasting, and Interactive Reporting\n\nScatter Podcast on UC Irvine News (2019-06-13)\nWinner of the Orange County Predictive Modeling Hackathon (2019-05-20)\nScatter Podcast mention on Forbes (2019-04-14)"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "In the past, I lost track of reality trying to track a gazillion links covering every data-science-friendly programming language under the sun. **shakes head** Bad idea. Since I program in R daily, I like to keep track of R and Posit / RStudio developments. I‚Äôm mostly going to share R resources that I find useful for analytics, statistical programming, machine learning, data science workflows, and web app development. I‚Äôm enjoying Python a lot more recently so I‚Äôll slowly build up this resources page with Python sub-topics that I find bookmark worthy.\nIn terms of the best place to start for getting into data analysis, I recommend learning SQL as this is by far the most widely used data querying language across the corporate and academic landscapes and if you master SQL, you‚Äôve mastered most of the transformations that are possible for tabular numeric data sets. Nonetheless, I will not cover SQL resources here as I rarely write raw SQL anymore. Instead, I use R to establish data warehouse connections and I query that raw data using the common tidyverse collection of R packages to execute SQL code in the back-end (via the dbplyr package).\nR and Python are open-source programming languages for statistical computing and graphics. These two languages have friendly online (and in-person) communities devoted to making data science easier to consume, easier to apply, and more effective at solving business problems. One of the things that I like most about both languages is the thousands of packages available making almost everything in R or Python a little easier from ETL, to method chaining, to developing predictive models and interactive web apps. I certainly welcome any suggestions that you might have for the lists below!"
  },
  {
    "objectID": "resources.html#r-books-classics",
    "href": "resources.html#r-books-classics",
    "title": "Resources",
    "section": "R Books: Classics",
    "text": "R Books: Classics\n\nR for Data Science: Phenomenal introduction to R, the RStudio IDE, and the tidyverse collection of packages\nAdvanced R: Covers concepts, methods, and advanced object-oriented structures for R\nMastering Shiny: Designed to teach the foundations of Shiny for web development and more advanced concepts such as the introduction of modules to the Shiny framework\nR Packages: The definitive reference point for R package development ‚Äúcovering workflow and process, alongside the presentation of all the important moving parts that make up an R package‚Äù"
  },
  {
    "objectID": "resources.html#r-books-applied-resources",
    "href": "resources.html#r-books-applied-resources",
    "title": "Resources",
    "section": "R Books: Applied Resources",
    "text": "R Books: Applied Resources\n\nTidy Modeling with R: Over the last few months, I‚Äôve learned a lot from this A to Z resource on predictive modeling workflows using the tidymodels framework\nDeep Learning with R, Second Edition: In-depth introduction to artificial intelligence and deep learning applications with R using the Keras library\nForecasting Principles and Practice, Third Edition: Said best by the author, ‚ÄúThe book is written for three audiences: (1) people finding themselves doing forecasting in business when they may not have had any formal training in the area; (2) undergraduate students studying business; (3) MBA students doing a forecasting elective.‚Äù\nRegression and Other Studies: Super applied textbook on advanced regression techniques, Bayesian inference, and causal inference\nSupervised Machine Learning for Text Analysis in R: Written by two Posit software engineers and incredible additions to their tidymodels team, Emil Hvitfeldt and Julia Silge, this book is a masterclass in natural language processing taking you from the basics of NLP to real-life applications including inference and prediction"
  },
  {
    "objectID": "resources.html#r-packages",
    "href": "resources.html#r-packages",
    "title": "Resources",
    "section": "R Packages",
    "text": "R Packages\n\ntidyverse: A collection of packages for data manipulation and functional programming (I use dplyr, stringr, and purrr on a daily basis)\ntidymodels: Hands-down my preferred collection of packages for building reproducible machine learning recipes, workflows, model tuning, model stacking, and cross-validation\ntidyverts: A collection of packages for time series analysis that comes out of Rob Hyndman‚Äôs lab\nDT: This is an R implementation of the popular DataTables JavaScript library that lets you build polished, configurable tables for use in web reports, slides, and Shiny apps\nbs4Dash: This R Shiny framework brings Bootstrap 4 + AdminLTE v3.1 dependencies to Shiny (including 1:1 support for shinydashboard functions) and it‚Äôs my go-to for developing enterprise-grade Shiny apps\nleaflet: R implementation of the popular Leaflet JavaScript library for developing interactive maps\nplotly: An extensive graphic library for creating interactive visualizations and 3D (WebGL) charts"
  },
  {
    "objectID": "resources.html#python-books",
    "href": "resources.html#python-books",
    "title": "Resources",
    "section": "Python Books",
    "text": "Python Books\n\nThe Quick Python Book, Third Edition: This book by Naomi Ceder is a few years old now (2018) but it‚Äôs the best end-to-end intro on Python that I‚Äôve yet read taking you from basic classes / structures to function writing to working with modules\nPython Data Science Handbook: Introduction to the core libraries essential for working with data in Python\nEffective Pandas: Patterns for Data Manipulation: Easy to follow tutorials, at your own pace, for mastering the popular Pandas library"
  },
  {
    "objectID": "resources.html#python-packages",
    "href": "resources.html#python-packages",
    "title": "Resources",
    "section": "Python Packages",
    "text": "Python Packages\n\nNumPy: Brings the computational power of C and Fortran to Python programmers for applying high-level mathematical functions to arrays and more\nPandas: This is the most popular package for data manipulation and analysis with extended operations available for tabular and time series data\nMatplotlib: A comprehensive library for creating static, animated, and interactive visualizations in Python\nscikit-learn: Built on top of NumPy, SciPy, and matplotlib, ‚Äúsklearn‚Äù makes the development of predictive analysis workflows a simple and reproducible process\nBeautiful Soup: The beautifulsoup4 library makes web scraping HTML and XML data a breeze\nStreamlit: Using pure Python, this package lets you build interactive web apps in minutes with no UI / front-end experience required"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "torch\n\n\nluz\n\n\n\n\nThe ‚Äòtorch for R‚Äô ecosystem is a collection of extensions for torch, an R framework for machine learning and artificial intelligence based on PyTorch.\n\n\n\n\n\n\nOct 22, 2021\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nrayshader\n\n\nraster\n\n\nosmdata\n\n\nsf\n\n\nggplot2\n\n\n\n\nIf you love beautiful 2D and 3D maps, now you can create your own with elevation data, the rayshader package, OpenStreetMap, and ggplot2.\n\n\n\n\n\n\nSep 7, 2021\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\ntibble\n\n\n\n\nInsightful read sourced from Towards Data Science on the history of the dataframe - From its origins in the S programming language, to R, to pandas for Python (to the tibble for R).\n\n\n\n\n\n\nMay 24, 2021\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nRStudio\n\n\nPackages\n\n\nR\n\n\nDT\n\n\n\n\nThe R package DT provides an R interface to the JavaScript library DataTables. R data objects (matrices or data frames) can be displayed as tables on HTML pages, and DataTables provides filtering, pagination, sorting, interactivity with Shiny, and many other features.\n\n\n\n\n\n\nMay 3, 2021\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nRStudio\n\n\nConference\n\n\ntidymodels\n\n\nR\n\n\n\n\nRStudio‚Äôs annual conference saw roughly 17,000 attendees for their first global, all-virtual, 24-hour event. I attended several sessions throughout the day and I‚Äôll highlight my favorite data bytes learned that day. I‚Äôll also share relatable content for better modeling with R.\n\n\n\n\n\n\nJan 24, 2021\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nRStudio\n\n\nPackages\n\n\nrsthemes\n\n\nR\n\n\nQuick Tips\n\n\n\n\nChange up your editor theme with {rsthemes}, a collection of themes to freshen up the RStudio IDE aesthetics.\n\n\n\n\n\n\nJan 19, 2021\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nJulia\n\n\nLinux\n\n\nRaspberry Pi\n\n\n\n\nI have been planning my 2021 goals and the Raspberry Pi 4 will help me kill a few birds with one stone.\n\n\n\n\n\n\nDec 31, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nPackages\n\n\nTutorial\n\n\n\n\nI followed this tutorial and created a package successfully. It took 45 minutes. Follow, this, guide, my fellow R friends!\n\n\n\n\n\n\nDec 19, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ntidyverse\n\n\nPackages\n\n\nR\n\n\nSQL\n\n\ndbplyr\n\n\ndplyr\n\n\n\n\ndbplyr, a database backend for dplyr, just released v2.0.0 today‚Ä¶ Awesome stuff here!\n\n\n\n\n\n\nNov 4, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPackages\n\n\nR\n\n\nTutorial\n\n\n\n\nThis is one of the best, most detailed, how-to guides for developing your own R package from A-to-Z\n\n\n\n\n\n\nSep 13, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nShiny\n\n\nR\n\n\nPackages\n\n\n\n\nWant to make your Shiny apps voice-interactive? Now it‚Äôs possible.\n\n\n\n\n\n\nJul 3, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPackages\n\n\nR\n\n\ntidyverse\n\n\ndplyr\n\n\n\n\nFinally getting around to trying out dplyr 1.0.0‚Ä¶ Love it!\n\n\n\n\n\n\nJun 20, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\nggplot2\n\n\nVisualizations\n\n\nR\n\n\n\n\nIt‚Äôs quite easy to manipulate raw data in a manner that ‚Äòproves‚Äô your point. For the sake of exploring this topic further, I‚Äôll analyze police killing data and present it in three different ways.\n\n\n\n\n\n\nApr 19, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nGaming\n\n\nblogdown\n\n\n\n\nIt‚Äôs an incredible time to be creative! I‚Äôve recently been re-learning Dungeons & Dragons with a group of friends and having so much fun in the process.\n\n\n\n\n\n\nApr 5, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ntidymodels\n\n\nPackages\n\n\n\n\nthemis contain extra steps for the recipes package for dealing with unbalanced data. The name themis is that of the ancient Greek goddess who is typically depicted with a balance.\n\n\n\n\n\n\nFeb 20, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nQuick Tips\n\n\n\n\nI was today years old when I learned that you could easily copy/paste R objects to your clipboard.\n\n\n\n\n\n\nFeb 8, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nRStudio\n\n\nR\n\n\nPython\n\n\nSuperDataScience\n\n\n\n\nMy eyes were opened to the world of analytics and data science in part through Kirill Eremenko and his amazing SuperDataScience podcast.\n\n\n\n\n\n\nFeb 4, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\nMachine Learning\n\n\nR\n\n\n\n\nClick for a high-level recap of k-fold cross validation as a resampling method.\n\n\n\n\n\n\nFeb 1, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\nCoffee\n\n\n\n\nCoffee lovers‚Ä¶ You might find this study fascinating. Finally, a model for the perfect cup of espresso!\n\n\n\n\n\n\nJan 22, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\nggplot2\n\n\nEarthquakes\n\n\n\n\nPuerto Rico has been experiencing atypical seismic activity since November 2019.\n\n\n\n\n\n\nJan 20, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nPackages\n\n\nDevelopment\n\n\n\n\nIf you are interested in developing your own R packages, this thorough A-to-Z tutorial resource is one to bookmark.\n\n\n\n\n\n\nJan 18, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nPackages\n\n\nQuick Tips\n\n\n\n\nSpending your days R but working closely with analysts and leadership that live in Excel? Get familiar with openxlsx.\n\n\n\n\n\n\nJan 9, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPrivacy\n\n\n\n\nThe California Consumer Privacy Act is set to go into effect on January 1, 2020. The CCPA, similar in nature to GDPR, will provide California residents with new consumer data rights.\n\n\n\n\n\n\nJan 4, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nGaming\n\n\n\n\nHoliday expectations‚Ä¶\n\n\n\n\n\n\nJan 1, 2020\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nLinux\n\n\nUbuntu\n\n\n\n\nThings not to do with Linux, learned the hard way, as we approach the end of 2019‚Ä¶\n\n\n\n\n\n\nDec 31, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\nR\n\n\nPackages\n\n\n\n\nDigging into Hands-On Machine Learning with R by Brad Boehmke, PhD, and Brandon Greenwell.\n\n\n\n\n\n\nDec 17, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSocial Good\n\n\n\n\nBay Area friends‚Ä¶ Next time your organization needs coffee for an event, quarterly meeting, all-hands, or lunch and learn, reach out to Java Joy!\n\n\n\n\n\n\nDec 15, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nEducation\n\n\n\n\nQuartz and Google‚Äôs Avinash Kaushik share insights comparing national education systems, test performance, and time spent on learning.\n\n\n\n\n\n\nDec 4, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nDevelopment\n\n\nPackages\n\n\nR\n\n\nblogdown\n\n\n\n\nLearn how to build and deploy a website with R, blogdown, GitHub, Hugo, and Netlify.\n\n\n\n\n\n\nNov 18, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nR Markdown\n\n\n\n\nIf you are an R user not using R Markdown, these tricks could be helpful for you.\n\n\n\n\n\n\nNov 11, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nShiny\n\n\nR\n\n\n\n\nMatt Dancho and Business Science introduce a new R Shiny Expert Developer course.\n\n\n\n\n\n\nNov 5, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nggplot2\n\n\nR\n\n\nVisualizations\n\n\n\n\nThe BBC News data science and visualization team published this great overview on their analytics and visualizations lessons learned over the course of 1.5 years.\n\n\n\n\n\n\nOct 22, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nVisualizations\n\n\n\n\nVisualization professionals present their best at the annual Information is Beautiful Awards show.\n\n\n\n\n\n\nOct 19, 2019\n\n\nJavier Orraca-Deatcu\n\n\n\n\n\n\nNo matching items"
  }
]