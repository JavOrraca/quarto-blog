---
title: "Databricks for R Users"
description: "Getting started with Databricks and R. Tips after two years of use."
date: 01-19-2025
categories: [brickster, httr2, mlflow, sparklyr]
image: databricks-logo.png
draft: true
---

# What is Databricks?

[Databricks](https://www.databricks.com/) is a cloud-based data and AI platform that uses [Apache Spark](https://spark.apache.org/) as its engine for automating large-scale data processing and machine learning. While Spark itself is an open-sourced project, its creators founded Databricks to lower the barrier to entry for organizations struggling with the management and configuration of Spark clusters, drivers, executors, nodes, dependencies, security, etc.

As Databricks attracted data engineers, analysts, machine learning practitioners, and Wall Street,^[In a Series J investment round in December 2024, Databricks raised $10 billion at a valuation of $62 billion. _Series J...!_] it evolved into the comprehensive AI platform we know today. The company open-sourced [MLflow](https://mlflow.org/) to support ML and GenAI experimentation, deployment, and model lifecycle management, and at the Databricks 2024 Data + AI Summit,^[Databricks gave my company 10 passes for their [Data + AI Summit](https://www.databricks.com/dataaisummit) and I was fortunate to attend with 16,000 other people in San Francisco, all under one roof. It was definitely the biggest conference I've ever attended and amazing to experience firsthand.] they open-sourced [Unity Catalog](https://www.unitycatalog.io/) to unify governance and security controls and simplify lakehouse architectures (data lakes + data warehouses) and make it easier to manage tabular data, unstructured data, and storage of data files, functions, and trained AI model objects.

Databricks can do a lot, a lot that I love, but make no mistake about a few things:

- Databricks is a Python-first platform
- Notebooks are everything on Databricks
- You can't launch robust IDEs on top of Databricks^[In the past, Databricks users could attach init scripts to their Spark clusters that'd allow them to launch VS Code or RStudio on top of Databricks. Databricks disabled this feature in 2024 gibven the future promises of the Databricks Lakehouse App Store (_I believe this is technically a component of the [Databricks Marketplace](https://marketplace.databricks.com/)_). The Lakehouse App Store will eventually (_hopefully_) have launchable IDEs on top of Databricks but the details and timing are nebulous. Databricks releases something like 32,164 new features per week so cut them some slack.]
- The future of Databricks looks like it'll be Serverless and this is concerning for R users^[_Surprise!_ Databricks Serverless [does not support R](https://docs.databricks.com/en/compute/serverless/limitations.html#general-limitations) and it's still too early to tell if R will eventually be supported on Serverless.]
- If you want to use RStudio or another IDE to remotely interact with your Databricks assets including your Spark clusters, Unity Catalog, MLflow, models, GenAI / LLM model-serving endpoints, etc., your best option is to rely on the Databricks Connect method^[R's [{sparklyr} documentation on the Databricks Connect method](https://spark.posit.co/deployment/databricks-connect.html) clearly explains how to setup your environment to remotely interact with your Spark clusters.] 

And let's take a quick sidebar to discuss notebooks... It's not that I don't like notebooks! I've used R Markdown and Quarto for years and as feature-rich as they might be for "scientific and technical publishing," you're either coding in Markdown or code chunks... It smells a lot like a notebook. ü§∑‚Äç‚ôÇÔ∏è This entire website was built with Quarto and R, _but_ I'm using an IDE to interact with the codebase for my website project. Imagine doing all of your development work in a Jupyter Notebook (an `.ipynb` file)... "Yuck" is the only appropriate sentiment here. Robust development on notebooks without an IDE feels wrong for this simple reason: It sucks. If you're new to Databricks in 2025 as a Python or R user, get ready for notebooks hell.

Okay, back to the good stuff. My goal with this post was less about giving you some kind of topical Databricks education and more about sharing personal best practices that I've adopted using Databricks as an R user. If you're an R user on Databricks and you feel stuck or you'd like to know my thoughts about other topics I don't cover, please reach out and I'll add to this post!

# Package Management

To understand packages on Databricks, let's take a step back to better understand what happens when a Spark cluster boots up. When a Databicks Spark cluster is launched with your runtime of choice, you're booting up a pre-configured and managed image that comes with a minimal Ubuntu CLI and tons of system, Python, and R dependencies pre-installed. These are referred to as "Databricks runtimes" and as of the original publication of this post, the most current Long Term Stable ("LTS") Databricks runtime version is the 15.4 runtime. Here is a general link to Databricks runtime release notes including notes for pre-release versions: <https://docs.databricks.com/en/release-notes/runtime/index.html>

I often visit Databricks docs to know which [Python](https://docs.databricks.com/en/release-notes/runtime/15.4lts-ml.html#python-libraries-on-cpu-clusters) and [R package versions](https://docs.databricks.com/en/release-notes/runtime/15.4lts.html#rlibraries) are pre-installed on my active Databricks runtime. The main problem is that it is expensive to keep a Spark cluster "always on" and whenever you re-launch your clusters, you need to re-install packages that aren't part of your Databricks runtime image. Your Databricks admins likely impose rules to automatically shutoff your Spark clusters if you are inactive for period of time (in my case, I think my clusters shutoff after one hour of inactivity). On the surface, this is a non-issue and even desireable, however, I find myself having to re-launch my Spark clusters multiple times per day due to conference calls, unscheduled technical / ad hoc work meetings, etc. It takes 8-10 minutes to launch a Spark ML cluster using Databricks 15.4 LTS runtime, and then only at that point is where I can begin the installation or upgrading of specific R and Python packages that I need to use. 

I haven't found a Gold Standard Rule for managing (or persisting) Python and R package dependencies on Databricks. You can modify your Spark cluster's configuration to install certain packages every time your cluster launches, but I prefer to simply re-install project-package dependencies after I launch my Spark cluster (and if you have a package manager that has R package binaries, install time will be a LOT faster). I'd also recommend against persisting R packages in Unity Catalog; This is a pattern that I'm using at work but this is also something I developed when I (incorrectly) thought Databricks runtimes were not capable of installing Python wheels / R binaries. 
